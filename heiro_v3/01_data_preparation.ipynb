{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Data Preparation & Anchor Extraction\n",
    "\n",
    "## Goal\n",
    "The objective of this notebook is to prepare the datasets required for our **Anchor-Guided Alignment** of Ancient Egyptian and Modern English.\n",
    "\n",
    "Unlike previous attempts that relied on massive unsupervised training or machine translation bridges, we will explicitly construct a set of **Anchors**‚Äîknown translations‚Äîthat will serve as the \"Rosetta Stone\" for our mathematical alignment later.\n",
    "\n",
    "## Steps\n",
    "1.  **Load Raw Data**: We will use the TLA (Thesaurus Linguae Aegyptiae) dataset, which contains hieroglyphic transliterations and their German translations.\n",
    "2.  **English Translation**: Since our target is English, we will use the pre-translated English versions (from the previous attempt's cache) to build our dictionary.\n",
    "3.  **Clean & Normalize**: We need to ensure the transliterations are consistent and the English text is clean.\n",
    "4.  **Construct Anchor Dictionary**: We will create a list of pairs `(hieroglyphic_word, english_word)` that we are confident in. These are our anchors.\n",
    "5.  **Export**: Save the clean corpora and the anchor dictionary for the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T17:25:39.937098Z",
     "iopub.status.busy": "2025-11-19T17:25:39.936874Z",
     "iopub.status.idle": "2025-11-19T17:25:40.322384Z",
     "shell.execute_reply": "2025-11-19T17:25:40.321890Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = \"data\"\n",
    "CACHE_FILE = os.path.join(DATA_DIR, \"german_english_translations.pkl\")\n",
    "HIEROGLYPHIC_CORPUS_FILE = os.path.join(DATA_DIR, \"hieroglyphic_corpus.txt\")\n",
    "ANCHOR_FILE = os.path.join(DATA_DIR, \"anchors.pkl\")\n",
    "CLEAN_CORPUS_FILE = os.path.join(DATA_DIR, \"clean_corpora.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "We are loading the `german_english_translations.pkl` file which contains:\n",
    "-   `hieroglyphic`: The transliterated Egyptian text.\n",
    "-   `german`: The original German translation from TLA.\n",
    "-   `english`: The English translation (machine translated from German in the previous attempt).\n",
    "\n",
    "We rely on this cached file to save time and resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T17:25:40.324205Z",
     "iopub.status.busy": "2025-11-19T17:25:40.324041Z",
     "iopub.status.idle": "2025-11-19T17:25:40.342349Z",
     "shell.execute_reply": "2025-11-19T17:25:40.341742Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data/german_english_translations.pkl...\n",
      "Loaded 12773 entries.\n",
      "\n",
      "Sample Entry:\n",
      "{'hieroglyphic': 'n·∏è (w)diÃØ r =s', 'german': '(es) werde zerrieben, (es) werde darauf gelegt.', 'hieroglyphs': 'ìê©ìèåìÄú ìÇß ìÇã ìã¥', 'lemmatization': '90880|n·∏è 51510|wdiÃØ 91901|r 10090|=s', 'date_not_before': '-1580', 'date_not_after': '-1539', 'english': 'It shall be crushed, and it shall be laid upon it.'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading data from {CACHE_FILE}...\")\n",
    "with open(CACHE_FILE, 'rb') as f:\n",
    "    raw_data = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded {len(raw_data)} entries.\")\n",
    "\n",
    "# Let's inspect a sample\n",
    "print(\"\\nSample Entry:\")\n",
    "print(raw_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning\n",
    "\n",
    "We need to process this list into a usable format. Specifically, we want to extract individual word pairs where possible, but since these are full sentences/phrases, we will first focus on the **sentences** for training the embeddings, and then try to extract **word-level anchors**.\n",
    "\n",
    "### 2.1 Corpus Preparation\n",
    "For `FastText` (Hieroglyphic) and our English model, we need clean lists of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T17:25:40.379111Z",
     "iopub.status.busy": "2025-11-19T17:25:40.378945Z",
     "iopub.status.idle": "2025-11-19T17:25:40.473854Z",
     "shell.execute_reply": "2025-11-19T17:25:40.473243Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Cleaning Data:   0%|      | 0/12773 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Cleaning Data: 100%|‚ñà| 12773/12773 [00:00<00:00, 161"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prepared 12773 parallel sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def clean_hieroglyphic(text):\n",
    "    \"\"\"Normalize hieroglyphic transliteration.\"\"\"\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    # Remove brackets and uncertain markers often found in TLA\n",
    "    text = re.sub(r'[\\[\\]\\(\\)\\?\\<\\>]', '', text)\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def clean_english(text):\n",
    "    \"\"\"Normalize English text.\"\"\"\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "hieroglyphic_sentences = []\n",
    "english_sentences = []\n",
    "\n",
    "for entry in tqdm(raw_data, desc=\"Cleaning Data\"):\n",
    "    h_clean = clean_hieroglyphic(entry.get('hieroglyphic', ''))\n",
    "    e_clean = clean_english(entry.get('english', ''))\n",
    "    \n",
    "    if h_clean and e_clean:\n",
    "        hieroglyphic_sentences.append(h_clean)\n",
    "        english_sentences.append(e_clean)\n",
    "\n",
    "print(f\"\\nPrepared {len(hieroglyphic_sentences)} parallel sentences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Anchor Extraction\n",
    "\n",
    "This is the most critical step for our \"Anchor-Guided\" approach. We need reliable pairs of `(egyptian_word, english_word)`.\n",
    "\n",
    "Since we have aligned sentences, we can use a simple heuristic: **Co-occurrence** or just use the **Lemmatization** data if available in the raw dump.\n",
    "\n",
    "Looking at the raw data structure from the previous analysis:\n",
    "```python\n",
    "{\n",
    "    'hieroglyphic': '...', \n",
    "    'german': '...', \n",
    "    'english': '...', \n",
    "    'lemmatization': 'lemma1|trans1 lemma2|trans2 ...'\n",
    "}\n",
    "```\n",
    "The `lemmatization` field seems to contain the gold mine! It likely maps specific hieroglyphic words to their IDs or lemmas. However, we might not have direct English translations for those lemmas, only the sentence translation.\n",
    "\n",
    "**Strategy**:\n",
    "1.  We will try to build a dictionary from the `lemmatization` if it contains readable text.\n",
    "2.  If `lemmatization` is just IDs, we will fall back to a frequency-based alignment on the sentences (e.g., if \"nfr\" appears in sentences with \"good\" 100 times, they are a pair).\n",
    "\n",
    "Let's inspect the `lemmatization` field of the first few entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T17:25:40.475392Z",
     "iopub.status.busy": "2025-11-19T17:25:40.475274Z",
     "iopub.status.idle": "2025-11-19T17:25:40.477773Z",
     "shell.execute_reply": "2025-11-19T17:25:40.477341Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting Lemmatization:\n",
      "Entry 0: 90880|n·∏è 51510|wdiÃØ 91901|r 10090|=s\n",
      "Entry 1: 78890|n 174900|·πØw 400007|m 10100|=sn\n",
      "Entry 2: 113110|·∏´Íú£ 400082|m 168810|t æ 110300|·∏•nq.t 162930|kÍú£ 107|Íú£pd 400055|n 25090|ÍûΩmÍú£·∏´.w 72420|ÍûΩm.ÍûΩ-r æ-≈°nÍú• 400161|Íûºmn-m-·∏•Íú£.t 66750|mÍú£Íú•-·∏´rw\n",
      "Entry 3: 40110|Íú•·∏•Íú•\n",
      "Entry 4: 49461|WsÍûΩr 800001|WnÍûΩs 67780|mÍûΩ 400055|n 10110|=k 28410|ÍûΩr.t-·∏§r.w 21680|ÍûΩÍú•b 400055|n 10110|=k 127770|sÍûΩ 91901|r 92560|r æ 10110|=k\n"
     ]
    }
   ],
   "source": [
    "print(\"Inspecting Lemmatization:\")\n",
    "for i in range(5):\n",
    "    print(f\"Entry {i}: {raw_data[i].get('lemmatization', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Building the Dictionary\n",
    "\n",
    "Assuming the lemmatization provides the hieroglyphic words, we can align them with the English words in the sentence. \n",
    "\n",
    "For this demonstration, we will use a **Frequency-Based Probabilistic Dictionary** approach (simplified IBM Model 1 idea):\n",
    "1.  Count word co-occurrences between Hieroglyphic words and English words in the parallel sentences.\n",
    "2.  Filter for high-confidence pairs.\n",
    "\n",
    "This is robust and doesn't require parsing complex lemma strings if they are messy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T17:25:40.479173Z",
     "iopub.status.busy": "2025-11-19T17:25:40.479053Z",
     "iopub.status.idle": "2025-11-19T17:25:40.960196Z",
     "shell.execute_reply": "2025-11-19T17:25:40.959725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building co-occurrence matrix...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Hieroglyphic words: 7174\n",
      "Unique English words: 7800\n"
     ]
    }
   ],
   "source": [
    "co_occurrence = Counter()\n",
    "h_freq = Counter()\n",
    "e_freq = Counter()\n",
    "\n",
    "print(\"Building co-occurrence matrix...\")\n",
    "for h_sent, e_sent in zip(hieroglyphic_sentences, english_sentences):\n",
    "    h_words = set(h_sent.split())\n",
    "    e_words = set(e_sent.split())\n",
    "    \n",
    "    for h in h_words:\n",
    "        h_freq[h] += 1\n",
    "        for e in e_words:\n",
    "            co_occurrence[(h, e)] += 1\n",
    "            e_freq[e] += 1\n",
    "\n",
    "print(f\"Unique Hieroglyphic words: {len(h_freq)}\")\n",
    "print(f\"Unique English words: {len(e_freq)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T17:25:40.961472Z",
     "iopub.status.busy": "2025-11-19T17:25:40.961379Z",
     "iopub.status.idle": "2025-11-19T17:25:40.995097Z",
     "shell.execute_reply": "2025-11-19T17:25:40.994545Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting anchors...\n",
      "Found 1362 potential anchors.\n",
      "\n",
      "Top 20 Anchors:\n",
      "=f -> the (prob: 0.54, count: 1592)\n",
      "=k -> you (prob: 0.61, count: 1425)\n",
      "m -> the (prob: 0.65, count: 1414)\n",
      "n -> the (prob: 0.56, count: 1288)\n",
      "·∏•r.w -> horus (prob: 0.99, count: 1172)\n",
      "wnÍûΩs -> unas (prob: 0.98, count: 572)\n",
      "·∏•r -> the (prob: 0.57, count: 485)\n",
      "r -> the (prob: 0.56, count: 448)\n",
      "n.ÍûΩ -> the (prob: 0.76, count: 404)\n",
      "pn -> this (prob: 0.87, count: 343)\n",
      "zÍú£ -> son (prob: 0.94, count: 307)\n",
      "pw -> the (prob: 0.50, count: 287)\n",
      "·∏èd-mdw -> words (prob: 0.92, count: 285)\n",
      "=ÍûΩ -> i (prob: 0.53, count: 275)\n",
      "ÍûΩr -> the (prob: 0.57, count: 269)\n",
      "=s -> the (prob: 0.49, count: 268)\n",
      "ppy -> pepi (prob: 1.00, count: 268)\n",
      "wsÍûΩr -> osiris (prob: 1.00, count: 262)\n",
      "nÍûΩ.t -> neith (prob: 0.98, count: 258)\n",
      "wsr.w -> osiris (prob: 1.00, count: 250)\n"
     ]
    }
   ],
   "source": [
    "# Calculate Pointwise Mutual Information (PMI) or just Conditional Probability P(e|h)\n",
    "# P(e|h) = count(h, e) / count(h)\n",
    "\n",
    "anchors = {}\n",
    "MIN_COUNT = 5\n",
    "CONFIDENCE_THRESHOLD = 0.3  # If a word translates to X 30% of the time, it's a candidate\n",
    "\n",
    "print(\"Extracting anchors...\")\n",
    "for (h, e), count in co_occurrence.items():\n",
    "    if count < MIN_COUNT:\n",
    "        continue\n",
    "        \n",
    "    # Conditional probability P(e|h)\n",
    "    prob = count / h_freq[h]\n",
    "    \n",
    "    if prob > CONFIDENCE_THRESHOLD:\n",
    "        # We keep the best translation for each hieroglyphic word\n",
    "        if h not in anchors or anchors[h]['prob'] < prob:\n",
    "            anchors[h] = {'english': e, 'prob': prob, 'count': count}\n",
    "\n",
    "print(f\"Found {len(anchors)} potential anchors.\")\n",
    "\n",
    "# Let's see some top anchors\n",
    "sorted_anchors = sorted(anchors.items(), key=lambda x: x[1]['count'], reverse=True)\n",
    "print(\"\\nTop 20 Anchors:\")\n",
    "for h, data in sorted_anchors[:20]:\n",
    "    print(f\"{h} -> {data['english']} (prob: {data['prob']:.2f}, count: {data['count']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Saving Data\n",
    "\n",
    "We will save:\n",
    "1.  The clean sentences (for training embeddings).\n",
    "2.  The anchor dictionary (for alignment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T17:25:40.996441Z",
     "iopub.status.busy": "2025-11-19T17:25:40.996349Z",
     "iopub.status.idle": "2025-11-19T17:25:41.003662Z",
     "shell.execute_reply": "2025-11-19T17:25:41.003204Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save Corpora\n",
    "with open(CLEAN_CORPUS_FILE, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'hieroglyphic': hieroglyphic_sentences,\n",
    "        'english': english_sentences\n",
    "    }, f)\n",
    "\n",
    "# Save Anchors\n",
    "anchor_list = [{'hieroglyphic': h, 'english': data['english']} for h, data in anchors.items()]\n",
    "with open(ANCHOR_FILE, 'wb') as f:\n",
    "    pickle.dump(anchor_list, f)\n",
    "\n",
    "print(\"Data saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
