{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Alignment & Analysis\n",
    "\n",
    "## Goal\n",
    "This is the final step. We have two separate embedding spaces:\n",
    "1.  **Hieroglyphic Space** ($H$)\n",
    "2.  **English Space** ($E$)\n",
    "\n",
    "And we have a set of **Anchors** (pairs of words $h_i, e_i$ that mean the same thing).\n",
    "\n",
    "We will use **Procrustes Analysis** to find a linear transformation (rotation matrix $R$) that maps $H$ onto $E$ such that:\n",
    "$$ v_{h_i} R \\approx v_{e_i} $$\n",
    "\n",
    "## Steps\n",
    "1.  Load models and anchors.\n",
    "2.  Construct alignment matrices from the anchors.\n",
    "3.  Compute the optimal rotation matrix $R$ using SVD.\n",
    "4.  Evaluate the alignment on a held-out test set.\n",
    "5.  **Discover**: Translate unknown hieroglyphic words into English!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T17:25:59.201398Z",
     "iopub.status.busy": "2025-11-19T17:25:59.201256Z",
     "iopub.status.idle": "2025-11-19T17:26:00.810278Z",
     "shell.execute_reply": "2025-11-19T17:26:00.809850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading anchors...\n",
      "Loaded 1362 anchors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from gensim.models import FastText, Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = \"data\"\n",
    "MODELS_DIR = \"models\"\n",
    "ANCHOR_FILE = os.path.join(DATA_DIR, \"anchors.pkl\")\n",
    "HIEROGLYPHIC_MODEL_FILE = os.path.join(MODELS_DIR, \"hieroglyphic_fasttext.model\")\n",
    "ENGLISH_MODEL_FILE = os.path.join(MODELS_DIR, \"english_word2vec.model\")\n",
    "\n",
    "print(\"Loading models...\")\n",
    "hier_model = FastText.load(HIEROGLYPHIC_MODEL_FILE)\n",
    "eng_model = Word2Vec.load(ENGLISH_MODEL_FILE)\n",
    "\n",
    "print(\"Loading anchors...\")\n",
    "with open(ANCHOR_FILE, 'rb') as f:\n",
    "    anchors = pickle.load(f)\n",
    "    \n",
    "print(f\"Loaded {len(anchors)} anchors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare Alignment Matrices\n",
    "\n",
    "We need to filter our anchors to ensure both words exist in their respective model vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T17:26:00.811744Z",
     "iopub.status.busy": "2025-11-19T17:26:00.811595Z",
     "iopub.status.idle": "2025-11-19T17:26:00.817917Z",
     "shell.execute_reply": "2025-11-19T17:26:00.817494Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered to 1362 valid anchors.\n",
      "X shape: (1362, 100), Y shape: (1362, 100)\n"
     ]
    }
   ],
   "source": [
    "valid_anchors = []\n",
    "X_list = []\n",
    "Y_list = []\n",
    "\n",
    "for anchor in anchors:\n",
    "    h_word = anchor['hieroglyphic']\n",
    "    e_word = anchor['english']\n",
    "    \n",
    "    # Check if words are in vocab\n",
    "    # FastText always has a vector (subwords), but Word2Vec might not\n",
    "    if e_word in eng_model.wv:\n",
    "        valid_anchors.append((h_word, e_word))\n",
    "        X_list.append(hier_model.wv[h_word])\n",
    "        Y_list.append(eng_model.wv[e_word])\n",
    "\n",
    "X = np.array(X_list)\n",
    "Y = np.array(Y_list)\n",
    "\n",
    "print(f\"Filtered to {len(valid_anchors)} valid anchors.\")\n",
    "print(f\"X shape: {X.shape}, Y shape: {Y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train/Test Split\n",
    "\n",
    "We'll use 80% of anchors to learn the rotation and 20% to test if it generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T17:26:00.819306Z",
     "iopub.status.busy": "2025-11-19T17:26:00.819214Z",
     "iopub.status.idle": "2025-11-19T17:26:00.822113Z",
     "shell.execute_reply": "2025-11-19T17:26:00.821660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 1089 pairs, Testing on 273 pairs.\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test, anchors_train, anchors_test = train_test_split(\n",
    "    X, Y, valid_anchors, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training on {len(X_train)} pairs, Testing on {len(X_test)} pairs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Procrustes Alignment\n",
    "\n",
    "We want to find orthogonal matrix $R$ that minimizes $||X_{train}R - Y_{train}||_F$.\n",
    "Solution: $R = UV^T$ where $U, \\Sigma, V^T = SVD(Y_{train}^T X_{train})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T17:26:00.823348Z",
     "iopub.status.busy": "2025-11-19T17:26:00.823224Z",
     "iopub.status.idle": "2025-11-19T17:26:00.836322Z",
     "shell.execute_reply": "2025-11-19T17:26:00.832499Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rotation matrix...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "def learn_rotation(X, Y):\n",
    "    # SVD of Y.T @ X\n",
    "    U, S, Vt = np.linalg.svd(Y.T @ X)\n",
    "    # R = U @ Vt\n",
    "    R = U @ Vt\n",
    "    return R\n",
    "\n",
    "print(\"Learning rotation matrix...\")\n",
    "R = learn_rotation(X_train, Y_train)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation\n",
    "\n",
    "For each test anchor $(h, e)$, we map $h$ to English space: $v_{pred} = v_h R$. \n",
    "Then we look at the top $k$ nearest neighbors in the English space. If $e$ is among them, it's a hit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T17:26:00.844453Z",
     "iopub.status.busy": "2025-11-19T17:26:00.844065Z",
     "iopub.status.idle": "2025-11-19T17:26:01.083420Z",
     "shell.execute_reply": "2025-11-19T17:26:01.075897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on Test Set...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 13.19%\n",
      "Top-5 Accuracy: 21.25%\n",
      "Top-10 Accuracy: 23.08%\n"
     ]
    }
   ],
   "source": [
    "def evaluate(X, anchors, R, top_k=10):\n",
    "    hits = 0\n",
    "    total = len(anchors)\n",
    "    \n",
    "    # Project all X to English space\n",
    "    X_projected = X @ R.T  # Note: X @ R.T is equivalent to (R @ X.T).T\n",
    "    \n",
    "    for i, (h_word, true_e_word) in enumerate(anchors):\n",
    "        pred_vec = X_projected[i]\n",
    "        \n",
    "        # Find nearest neighbors in English model\n",
    "        # We use the model's built-in most_similar which is efficient\n",
    "        similar = eng_model.wv.most_similar([pred_vec], topn=top_k)\n",
    "        candidates = [w for w, score in similar]\n",
    "        \n",
    "        if true_e_word in candidates:\n",
    "            hits += 1\n",
    "            \n",
    "    accuracy = hits / total\n",
    "    return accuracy\n",
    "\n",
    "print(\"Evaluating on Test Set...\")\n",
    "acc_1 = evaluate(X_test, anchors_test, R, top_k=1)\n",
    "acc_5 = evaluate(X_test, anchors_test, R, top_k=5)\n",
    "acc_10 = evaluate(X_test, anchors_test, R, top_k=10)\n",
    "\n",
    "print(f\"Top-1 Accuracy: {acc_1:.2%}\")\n",
    "print(f\"Top-5 Accuracy: {acc_5:.2%}\")\n",
    "print(f\"Top-10 Accuracy: {acc_10:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Discovery & Translation\n",
    "\n",
    "Now for the fun part. Let's define a translation function and try it on some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T17:26:01.096124Z",
     "iopub.status.busy": "2025-11-19T17:26:01.092526Z",
     "iopub.status.idle": "2025-11-19T17:26:01.122904Z",
     "shell.execute_reply": "2025-11-19T17:26:01.114685Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translation for 'nfr':\n",
      "  -> tjetu (0.487)\n",
      "  -> the (0.484)\n",
      "  -> of (0.473)\n",
      "  -> behdeti (0.455)\n",
      "  -> colorful (0.453)\n",
      "\n",
      "Translation for 'pr-aa':\n",
      "  -> house (0.597)\n",
      "  -> domestic (0.574)\n",
      "  -> elevator (0.558)\n",
      "  -> hutsahaneb (0.535)\n",
      "  -> annual (0.529)\n",
      "\n",
      "Translation for 'ankh':\n",
      "  -> norm (0.307)\n",
      "  -> gloss (0.275)\n",
      "  -> self (0.267)\n",
      "  -> wiped (0.244)\n",
      "  -> avoid (0.243)\n",
      "\n",
      "Translation for 'maat':\n",
      "  -> anchkai (0.355)\n",
      "  -> powers (0.306)\n",
      "  -> rises (0.301)\n",
      "  -> trembles (0.283)\n",
      "  -> removed (0.277)\n",
      "\n",
      "Translation for 'ra':\n",
      "  -> bench (0.248)\n",
      "  -> anchkai (0.230)\n",
      "  -> administrations (0.207)\n",
      "  -> from (0.203)\n",
      "  -> hall (0.197)\n",
      "\n",
      "Translation for 'suten':\n",
      "  -> incorruptible (0.316)\n",
      "  -> tetis (0.298)\n",
      "  -> unas (0.281)\n",
      "  -> provides (0.273)\n",
      "  -> dwellers (0.269)\n",
      "\n",
      "Translation for 'netjer':\n",
      "  -> wiped (0.310)\n",
      "  -> lit (0.297)\n",
      "  -> make (0.272)\n",
      "  -> fall (0.254)\n",
      "  -> mouth (0.252)\n"
     ]
    }
   ],
   "source": [
    "def translate(h_word, top_k=5):\n",
    "    # Get vector (FastText handles OOV)\n",
    "    vec = hier_model.wv[h_word]\n",
    "    # Project\n",
    "    proj_vec = vec @ R.T\n",
    "    # Find neighbors\n",
    "    similar = eng_model.wv.most_similar([proj_vec], topn=top_k)\n",
    "    \n",
    "    print(f\"\\nTranslation for '{h_word}':\")\n",
    "    for w, score in similar:\n",
    "        print(f\"  -> {w} ({score:.3f})\")\n",
    "\n",
    "# Famous words\n",
    "translate(\"nfr\")       # Good/Beautiful\n",
    "translate(\"pr-aa\")     # Pharaoh (Great House)\n",
    "translate(\"ankh\")      # Life\n",
    "translate(\"maat\")      # Truth/Order\n",
    "translate(\"ra\")        # Sun God\n",
    "translate(\"suten\")     # King (nswt)\n",
    "translate(\"netjer\")    # God (nTr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
