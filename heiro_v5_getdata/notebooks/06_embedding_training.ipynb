{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 3: Embedding Training\n",
                "\n",
                "## Goal\n",
                "Train FastText embeddings on our 104k hieroglyphic corpus and prepare for alignment with English.\n",
                "\n",
                "## Strategy\n",
                "Following V3's successful approach, but with 10x more data:\n",
                "1. Train FastText on hieroglyphic transliterations\n",
                "2. Load pre-trained English embeddings (or train on Wikipedia)\n",
                "3. Verify anchor coverage in both embedding spaces\n",
                "4. Save embeddings for alignment\n",
                "\n",
                "## Key Improvements over V3\n",
                "- **10x more training data** (104k vs 12k texts)\n",
                "- **4x more anchors** (8.5k vs 1.3k pairs)\n",
                "- **Hieroglyphs column** for additional context\n",
                "- **Larger vocabulary** (~20k vs 7k unique words)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Libraries loaded successfully\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "\n",
                "import pandas as pd\n",
                "import pickle\n",
                "import json\n",
                "from pathlib import Path\n",
                "from gensim.models import FastText\n",
                "from gensim.models import KeyedVectors\n",
                "import numpy as np\n",
                "from tqdm import tqdm\n",
                "\n",
                "print(\"Libraries loaded successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Corpus and Anchors"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Corpus size: 104,426 texts\n",
                        "\n",
                        "Columns: ['transliteration', 'translation', 'source', 'metadata', 'hieroglyphs', 'transliteration_clean']\n"
                    ]
                }
            ],
            "source": [
                "# Load hieroglyphic corpus\n",
                "corpus_path = Path('../data/processed/hieroglyphic_corpus_full.tsv')\n",
                "df = pd.read_csv(corpus_path, sep='\\t')\n",
                "\n",
                "print(f\"Corpus size: {len(df):,} texts\")\n",
                "print(f\"\\nColumns: {df.columns.tolist()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded 8,541 English anchors\n",
                        "\n",
                        "Sample anchors:\n",
                        "  n               → der             (conf: 0.34)\n",
                        "  m               → der             (conf: 0.37)\n",
                        "  =f              → er              (conf: 0.38)\n",
                        "  =k              → du              (conf: 0.45)\n",
                        "  =j              → ich             (conf: 0.60)\n"
                    ]
                }
            ],
            "source": [
                "# Load English anchors\n",
                "anchors_path = Path('../data/processed/english_anchors.pkl')\n",
                "with open(anchors_path, 'rb') as f:\n",
                "    anchors = pickle.load(f)\n",
                "\n",
                "print(f\"Loaded {len(anchors):,} English anchors\")\n",
                "print(f\"\\nSample anchors:\")\n",
                "for i in range(5):\n",
                "    a = anchors[i]\n",
                "    print(f\"  {a['hieroglyphic']:15s} → {a['english']:15s} (conf: {a['confidence']:.2f})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Prepare Training Sentences\n",
                "\n",
                "FastText needs a list of tokenized sentences."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Preparing sentences: 100%|██████████| 104426/104426 [00:00<00:00, 727945.12it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Total sentences: 104,426\n",
                        "Sample sentence: ['nḏ', '(w)di̯', 'r', '=s']...\n",
                        "\n",
                        "Vocabulary size: 85,013 unique words\n",
                        "Total tokens: 872,070\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Extract sentences from transliterations\n",
                "sentences = []\n",
                "\n",
                "for text in tqdm(df['transliteration_clean'].dropna(), desc=\"Preparing sentences\"):\n",
                "    if isinstance(text, str) and text.strip():\n",
                "        # Split into words\n",
                "        words = text.split()\n",
                "        if len(words) > 0:\n",
                "            sentences.append(words)\n",
                "\n",
                "print(f\"\\nTotal sentences: {len(sentences):,}\")\n",
                "print(f\"Sample sentence: {sentences[0][:10]}...\")\n",
                "\n",
                "# Vocabulary stats\n",
                "all_words = [word for sent in sentences for word in sent]\n",
                "vocab = set(all_words)\n",
                "print(f\"\\nVocabulary size: {len(vocab):,} unique words\")\n",
                "print(f\"Total tokens: {len(all_words):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Train FastText Model\n",
                "\n",
                "Using similar hyperparameters to V3, but adjusted for larger corpus."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training FastText model...\n",
                        "Parameters: dim=300, window=5, min_count=5, epochs=10\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "✓ Training complete!\n",
                        "Vocabulary size: 11,974 words\n"
                    ]
                }
            ],
            "source": [
                "# FastText hyperparameters\n",
                "VECTOR_SIZE = 300      # Embedding dimension (same as V3)\n",
                "WINDOW = 5             # Context window\n",
                "MIN_COUNT = 5          # Minimum word frequency\n",
                "EPOCHS = 10            # Training epochs\n",
                "SG = 1                 # Skip-gram (1) vs CBOW (0)\n",
                "\n",
                "print(\"Training FastText model...\")\n",
                "print(f\"Parameters: dim={VECTOR_SIZE}, window={WINDOW}, min_count={MIN_COUNT}, epochs={EPOCHS}\")\n",
                "\n",
                "model = FastText(\n",
                "    sentences=sentences,\n",
                "    vector_size=VECTOR_SIZE,\n",
                "    window=WINDOW,\n",
                "    min_count=MIN_COUNT,\n",
                "    epochs=EPOCHS,\n",
                "    sg=SG,\n",
                "    workers=4\n",
                ")\n",
                "\n",
                "print(\"\\n✓ Training complete!\")\n",
                "print(f\"Vocabulary size: {len(model.wv):,} words\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Verify Anchor Coverage\n",
                "\n",
                "Check how many of our anchors are in the trained embedding space."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Anchor Coverage:\n",
                        "  Found: 8,541 / 8,541 (100.0%)\n",
                        "  Missing: 0\n"
                    ]
                }
            ],
            "source": [
                "# Check anchor coverage\n",
                "hieroglyphic_words = [a['hieroglyphic'] for a in anchors]\n",
                "found = [w for w in hieroglyphic_words if w in model.wv]\n",
                "missing = [w for w in hieroglyphic_words if w not in model.wv]\n",
                "\n",
                "print(f\"Anchor Coverage:\")\n",
                "print(f\"  Found: {len(found):,} / {len(hieroglyphic_words):,} ({len(found)/len(hieroglyphic_words)*100:.1f}%)\")\n",
                "print(f\"  Missing: {len(missing):,}\")\n",
                "\n",
                "if len(missing) > 0:\n",
                "    print(f\"\\nTop 10 missing anchors:\")\n",
                "    missing_with_freq = [(w, next((a['frequency'] for a in anchors if a['hieroglyphic'] == w), 0)) for w in missing[:10]]\n",
                "    for w, freq in sorted(missing_with_freq, key=lambda x: x[1], reverse=True)[:10]:\n",
                "        print(f\"  {w:15s} (freq: {freq:,})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Test Embeddings\n",
                "\n",
                "Quick sanity check: find similar words to key anchors."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Similar words test:\n",
                        "======================================================================\n",
                        "\n",
                        "wsjr:\n",
                        "  wsj             (similarity: 0.833)\n",
                        "  〈wsjr〉          (similarity: 0.813)\n",
                        "  wsjr-ḫnt(,j)-jmn,tt (similarity: 0.789)\n",
                        "  wsjr-ḫnt,j-jmn,tj.w (similarity: 0.788)\n",
                        "  wsjr-wnn-nfr    (similarity: 0.780)\n",
                        "\n",
                        "ḥr,w:\n",
                        "  ḥr,wt           (similarity: 0.813)\n",
                        "  +ḥr,w           (similarity: 0.797)\n",
                        "  〈ḥr,w〉          (similarity: 0.743)\n",
                        "  {ḥr,w}          (similarity: 0.741)\n",
                        "  ḥr,w-j          (similarity: 0.729)\n",
                        "\n",
                        "ppy:\n",
                        "  nfr-kꜣ-rꜥw      (similarity: 0.876)\n",
                        "  nfr-kꜣ-[rꜥw]    (similarity: 0.820)\n",
                        "  ⸢nfr-kꜣ-rꜥw⸣    (similarity: 0.818)\n",
                        "  sḫm-kꜣ-rꜥw      (similarity: 0.782)\n",
                        "  [nfr-kꜣ-rꜥw]    (similarity: 0.781)\n",
                        "\n",
                        "zꜣ:\n",
                        "  zꜣt             (similarity: 0.646)\n",
                        "  zꜣš             (similarity: 0.614)\n",
                        "  zꜣ-z            (similarity: 0.598)\n",
                        "  zꜣb             (similarity: 0.596)\n",
                        "  sms,w           (similarity: 0.595)\n",
                        "\n",
                        "nṯr:\n",
                        "  nṯri̯           (similarity: 0.669)\n",
                        "  nṯr.j           (similarity: 0.662)\n",
                        "  nṯ              (similarity: 0.643)\n",
                        "  〈nṯr〉           (similarity: 0.642)\n",
                        "  nṯrj            (similarity: 0.637)\n"
                    ]
                }
            ],
            "source": [
                "# Test similar words\n",
                "test_words = ['wsjr', 'ḥr,w', 'ppy', 'zꜣ', 'nṯr']  # osiris, horus, pepi, son, god\n",
                "\n",
                "print(\"Similar words test:\")\n",
                "print(\"=\"*70)\n",
                "for word in test_words:\n",
                "    if word in model.wv:\n",
                "        similar = model.wv.most_similar(word, topn=5)\n",
                "        print(f\"\\n{word}:\")\n",
                "        for sim_word, score in similar:\n",
                "            print(f\"  {sim_word:15s} (similarity: {score:.3f})\")\n",
                "    else:\n",
                "        print(f\"\\n{word}: NOT IN VOCABULARY\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Save Hieroglyphic Embeddings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "✓ Saved full model to ../data/processed/hieroglyphic_fasttext.model\n",
                        "✓ Saved word vectors to ../data/processed/hieroglyphic_vectors.kv\n"
                    ]
                }
            ],
            "source": [
                "# Save the model\n",
                "model_path = Path('../data/processed/hieroglyphic_fasttext.model')\n",
                "model.save(str(model_path))\n",
                "print(f\"✓ Saved full model to {model_path}\")\n",
                "\n",
                "# Save just the word vectors (smaller, faster to load)\n",
                "wv_path = Path('../data/processed/hieroglyphic_vectors.kv')\n",
                "model.wv.save(str(wv_path))\n",
                "print(f\"✓ Saved word vectors to {wv_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. English Embeddings\n",
                "\n",
                "We have two options:\n",
                "1. **Use pre-trained** (e.g., GloVe, Word2Vec from Google)\n",
                "2. **Train on Wikipedia** (what V3 did)\n",
                "\n",
                "For now, let's use **pre-trained GloVe** (faster, proven quality)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Option 1: Download pre-trained GloVe embeddings\n",
                "# Download from: https://nlp.stanford.edu/projects/glove/\n",
                "# We'll use glove.6B.300d.txt (300 dimensions, 6B tokens)\n",
                "\n",
                "print(\"To use pre-trained English embeddings:\")\n",
                "print(\"1. Download GloVe: wget http://nlp.stanford.edu/data/glove.6B.zip\")\n",
                "print(\"2. Extract: unzip glove.6B.zip\")\n",
                "print(\"3. Move glove.6B.300d.txt to ../data/processed/\")\n",
                "print(\"\\nOr we can train on Wikipedia (slower, but custom to our needs)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Next Steps: Phase 4 - Alignment\n",
                "\n",
                "Once we have both embedding spaces:\n",
                "1. Load hieroglyphic and English embeddings\n",
                "2. Extract anchor vectors from both spaces\n",
                "3. Apply Orthogonal Procrustes alignment\n",
                "4. Evaluate on V3's test set\n",
                "5. Discover new meanings (Anubis-type connections!)\n",
                "\n",
                "Create `07_procrustes_alignment.ipynb` next."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python (spontaneous-remission)",
            "language": "python",
            "name": "spontaneous-remission"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
