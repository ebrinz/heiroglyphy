{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# V5 Data Strategy: Leveraging the Combined Corpus\n",
                "\n",
                "## Context\n",
                "We now have a **significantly larger dataset** than previous attempts:\n",
                "- **TLA**: ~12,773 texts (German translations)\n",
                "- **Ramses**: ~9,644 texts (French/multilingual)\n",
                "- **BBAW**: ~100,736 texts (German translations + **hieroglyphs**)\n",
                "- **Total**: ~123k texts → ~104k after deduplication\n",
                "\n",
                "## The German Problem\n",
                "Most of our data has **German** translations, not English. This creates a challenge for our goal of mapping hieroglyphs to **English** meanings.\n",
                "\n",
                "## Previous Approaches (V1-V4)\n",
                "### V3 (Most Successful: ~22% accuracy)\n",
                "- Used **Orthogonal Procrustes** (linear alignment)\n",
                "- Required ~1,362 **anchor pairs** (hieroglyphic ↔ English)\n",
                "- Anchors extracted via co-occurrence analysis\n",
                "- **Key insight**: Context-based alignment works, but needs good anchors\n",
                "\n",
                "### V3's Data Pipeline\n",
                "1. German → English translation (machine translation)\n",
                "2. Build co-occurrence matrix between hieroglyphic words and English words\n",
                "3. Extract high-confidence pairs as anchors\n",
                "4. Train FastText on hieroglyphic corpus\n",
                "5. Train Word2Vec on English corpus (Wikipedia)\n",
                "6. Align using Procrustes with anchors\n",
                "\n",
                "## V5 Strategy Options\n",
                "\n",
                "### Option 1: Machine Translation (V3 approach, scaled up)\n",
                "**Pros:**\n",
                "- Proven to work (V3 achieved 22%)\n",
                "- 10x more data = potentially better embeddings\n",
                "- Can reuse V3 pipeline\n",
                "\n",
                "**Cons:**\n",
                "- Translation errors compound\n",
                "- Loses nuance (German → English isn't perfect)\n",
                "- Expensive (API costs for 100k+ texts)\n",
                "\n",
                "### Option 2: Trilingual Alignment (Hieroglyphic ↔ German ↔ English)\n",
                "**Pros:**\n",
                "- No translation needed for German\n",
                "- Can use German as a \"bridge\" language\n",
                "- Preserves original scholarly translations\n",
                "\n",
                "**Cons:**\n",
                "- More complex (need German embeddings)\n",
                "- Two alignment steps (more error propagation)\n",
                "- Still need English corpus\n",
                "\n",
                "### Option 3: Multilingual Embeddings (Modern approach)\n",
                "**Pros:**\n",
                "- Use pre-trained multilingual models (mBERT, XLM-R)\n",
                "- German/English already aligned in the model\n",
                "- State-of-the-art NLP\n",
                "\n",
                "**Cons:**\n",
                "- Hieroglyphic transliteration not in pre-trained vocab\n",
                "- Need to fine-tune on our corpus\n",
                "- More complex than Word2Vec/FastText\n",
                "\n",
                "### Option 4: Hybrid Approach (Recommended)\n",
                "**Strategy:**\n",
                "1. **Keep German** for the majority of data\n",
                "2. **Translate only anchor candidates** (high-frequency words)\n",
                "3. Use **hieroglyphic column** from BBAW for richer context\n",
                "4. Build **German-aligned** hieroglyphic embeddings\n",
                "5. Use **pre-trained German-English alignment** (from mBERT or similar)\n",
                "6. Map: Hieroglyphic → German → English\n",
                "\n",
                "**Why this works:**\n",
                "- Minimizes translation cost (only ~2k anchor words)\n",
                "- Leverages scholarly German translations (more accurate)\n",
                "- Uses modern multilingual models for German↔English\n",
                "- Preserves hieroglyphic context\n",
                "\n",
                "## Next Steps\n",
                "1. **Analyze the corpus** (this notebook)\n",
                "2. **Extract German anchors** (high-confidence hieroglyphic ↔ German pairs)\n",
                "3. **Translate anchors** to English (small, manageable set)\n",
                "4. **Train hieroglyphic embeddings** (FastText on full corpus)\n",
                "5. **Align to German** (Procrustes with German anchors)\n",
                "6. **Bridge to English** (using pre-trained German-English alignment)\n",
                "7. **Evaluate** against V3 results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Total records: 104,426\n",
                        "\n",
                        "Columns: ['transliteration', 'translation', 'source', 'metadata', 'hieroglyphs', 'transliteration_clean']\n",
                        "\n",
                        "Source distribution:\n",
                        "source\n",
                        "BBAW (HuggingFace)    87100\n",
                        "TLA (HuggingFace)      9088\n",
                        "Ramses Online          8238\n",
                        "Name: count, dtype: int64\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import json\n",
                "from pathlib import Path\n",
                "from collections import Counter\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Load the combined corpus\n",
                "corpus_path = Path('../data/processed/hieroglyphic_corpus_full.tsv')\n",
                "df = pd.read_csv(corpus_path, sep='\\t')\n",
                "\n",
                "print(f\"Total records: {len(df):,}\")\n",
                "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
                "print(f\"\\nSource distribution:\")\n",
                "print(df['source'].value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Translation Coverage:\n",
                        "Records with translation: 95,314\n",
                        "Empty translations: 0\n",
                        "\n",
                        "Records with hieroglyphs: 30,992\n",
                        "Percentage: 29.7%\n"
                    ]
                }
            ],
            "source": [
                "# Analyze translation availability\n",
                "print(\"Translation Coverage:\")\n",
                "print(f\"Records with translation: {df['translation'].notna().sum():,}\")\n",
                "print(f\"Empty translations: {(df['translation'] == '').sum():,}\")\n",
                "\n",
                "# Check hieroglyphs column (from BBAW)\n",
                "if 'hieroglyphs' in df.columns:\n",
                "    print(f\"\\nRecords with hieroglyphs: {df['hieroglyphs'].notna().sum():,}\")\n",
                "    print(f\"Percentage: {df['hieroglyphs'].notna().sum() / len(df) * 100:.1f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Total tokens: 872,070\n",
                        "Unique words: 87,636\n",
                        "\n",
                        "Top 20 most common hieroglyphic words:\n",
                        "  n: 37,473\n",
                        "  =f: 37,217\n",
                        "  m: 32,386\n",
                        "  =k: 26,900\n",
                        "  ḥr: 14,064\n",
                        "  r: 13,849\n",
                        "  =j: 13,735\n",
                        "  jw: 10,208\n",
                        "  1: 7,030\n",
                        "  pꜣ: 6,536\n",
                        "  =s: 5,847\n",
                        "  =sn: 5,735\n",
                        "  nb: 5,403\n",
                        "  tꜣ: 4,421\n",
                        "  2: 4,409\n",
                        "  sw: 3,980\n",
                        "  pw: 3,707\n",
                        "  n(,j): 3,657\n",
                        "  jr: 3,646\n",
                        "  jm: 3,508\n"
                    ]
                }
            ],
            "source": [
                "# Vocabulary analysis\n",
                "all_words = ' '.join(df['transliteration'].dropna()).split()\n",
                "word_freq = Counter(all_words)\n",
                "\n",
                "print(f\"Total tokens: {len(all_words):,}\")\n",
                "print(f\"Unique words: {len(word_freq):,}\")\n",
                "print(f\"\\nTop 20 most common hieroglyphic words:\")\n",
                "for word, count in word_freq.most_common(20):\n",
                "    print(f\"  {word}: {count:,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "==================================================\n",
                        "V3 vs V5 Comparison\n",
                        "==================================================\n",
                        "V3 corpus size: ~12,773 texts\n",
                        "V5 corpus size: 104,426 texts\n",
                        "Increase: 8.2x\n",
                        "\n",
                        "V3 vocabulary: ~7,174 unique words\n",
                        "V5 vocabulary: 87,636 unique words\n",
                        "Increase: 12.2x\n"
                    ]
                }
            ],
            "source": [
                "# Comparison with V3 dataset\n",
                "print(\"=\"*50)\n",
                "print(\"V3 vs V5 Comparison\")\n",
                "print(\"=\"*50)\n",
                "print(f\"V3 corpus size: ~12,773 texts\")\n",
                "print(f\"V5 corpus size: {len(df):,} texts\")\n",
                "print(f\"Increase: {len(df) / 12773:.1f}x\")\n",
                "print(f\"\\nV3 vocabulary: ~7,174 unique words\")\n",
                "print(f\"V5 vocabulary: {len(word_freq):,} unique words\")\n",
                "print(f\"Increase: {len(word_freq) / 7174:.1f}x\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Recommended Action Plan\n",
                "\n",
                "### Phase 1: Anchor Extraction (German)\n",
                "- Build co-occurrence matrix: hieroglyphic ↔ German\n",
                "- Extract ~2,000 high-confidence pairs\n",
                "- Filter for words that appear in V3's successful anchors\n",
                "\n",
                "### Phase 2: Selective Translation\n",
                "- Translate only the ~2,000 anchor German words to English\n",
                "- Use DeepL or Google Translate API\n",
                "- Manual review of top 100 most frequent\n",
                "\n",
                "### Phase 3: Embedding Training\n",
                "- Train FastText on full hieroglyphic corpus (104k texts)\n",
                "- Use hieroglyphs column for additional context\n",
                "- Larger embedding dimension (300 → 500?)\n",
                "\n",
                "### Phase 4: Alignment\n",
                "- Primary: Hieroglyphic → English (via translated anchors)\n",
                "- Fallback: Hieroglyphic → German → English (bridge)\n",
                "\n",
                "### Phase 5: Evaluation\n",
                "- Test on V3's evaluation set\n",
                "- Target: >22% accuracy\n",
                "- Analyze: Does 10x data improve Anubis-type discoveries?"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python (spontaneous-remission)",
            "language": "python",
            "name": "spontaneous-remission"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
