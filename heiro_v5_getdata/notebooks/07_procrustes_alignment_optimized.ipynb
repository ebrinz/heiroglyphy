{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 4: Procrustes Alignment (Optimized)\n",
                "\n",
                "## Goal\n",
                "Align hieroglyphic and English embedding spaces using Orthogonal Procrustes.\n",
                "\n",
                "**Optimization**: Vectorized similarity computation for 100x speedup!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ“ Libraries loaded\n"
                    ]
                }
            ],
            "source": [
                "import numpy as np\n",
                "import pickle\n",
                "import json\n",
                "from pathlib import Path\n",
                "from gensim.models import KeyedVectors\n",
                "from scipy.linalg import orthogonal_procrustes\n",
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "import pandas as pd\n",
                "from tqdm import tqdm\n",
                "\n",
                "print(\"âœ“ Libraries loaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Embeddings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading hieroglyphic embeddings...\n",
                        "âœ“ Loaded 11,974 hieroglyphic vectors\n",
                        "\n",
                        "Loading GloVe embeddings...\n",
                        "âœ“ Loaded 400,000 English vectors\n",
                        "  English matrix shape: (400000, 300)\n"
                    ]
                }
            ],
            "source": [
                "# Load hieroglyphic embeddings\n",
                "print(\"Loading hieroglyphic embeddings...\")\n",
                "hier_path = Path('../data/processed/hieroglyphic_vectors.kv')\n",
                "hier_wv = KeyedVectors.load(str(hier_path), mmap='r')\n",
                "print(f\"âœ“ Loaded {len(hier_wv):,} hieroglyphic vectors\")\n",
                "\n",
                "# Load GloVe English embeddings\n",
                "print(\"\\nLoading GloVe embeddings...\")\n",
                "glove_path = Path('../data/processed/glove.6B.300d.txt')\n",
                "\n",
                "eng_words = []\n",
                "eng_vecs = []\n",
                "\n",
                "with open(glove_path, 'r', encoding='utf-8') as f:\n",
                "    for line in f:\n",
                "        parts = line.strip().split()\n",
                "        eng_words.append(parts[0])\n",
                "        eng_vecs.append([float(x) for x in parts[1:]])\n",
                "\n",
                "# Convert to numpy array for vectorized operations\n",
                "eng_matrix = np.array(eng_vecs)\n",
                "eng_word_to_idx = {word: i for i, word in enumerate(eng_words)}\n",
                "\n",
                "print(f\"âœ“ Loaded {len(eng_words):,} English vectors\")\n",
                "print(f\"  English matrix shape: {eng_matrix.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Anchors and Extract Vectors"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded 8,541 anchor pairs\n"
                    ]
                }
            ],
            "source": [
                "# Load English anchors\n",
                "anchors_path = Path('../data/processed/english_anchors.pkl')\n",
                "with open(anchors_path, 'rb') as f:\n",
                "    anchors = pickle.load(f)\n",
                "\n",
                "print(f\"Loaded {len(anchors):,} anchor pairs\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Valid anchor pairs: 7,471 / 8,541 (87.5%)\n",
                        "\n",
                        "Anchor matrix shapes:\n",
                        "  X (hieroglyphic): (7471, 300)\n",
                        "  Y (English): (7471, 300)\n"
                    ]
                }
            ],
            "source": [
                "# Extract anchor vectors\n",
                "X_list = []  # Hieroglyphic vectors\n",
                "Y_list = []  # English vectors\n",
                "valid_anchors = []\n",
                "\n",
                "for anchor in anchors:\n",
                "    h_word = anchor['hieroglyphic']\n",
                "    e_word = anchor['english']\n",
                "    \n",
                "    if h_word in hier_wv and e_word in eng_word_to_idx:\n",
                "        X_list.append(hier_wv[h_word])\n",
                "        Y_list.append(eng_matrix[eng_word_to_idx[e_word]])\n",
                "        valid_anchors.append(anchor)\n",
                "\n",
                "X = np.array(X_list)\n",
                "Y = np.array(Y_list)\n",
                "\n",
                "print(f\"Valid anchor pairs: {len(valid_anchors):,} / {len(anchors):,} ({len(valid_anchors)/len(anchors)*100:.1f}%)\")\n",
                "print(f\"\\nAnchor matrix shapes:\")\n",
                "print(f\"  X (hieroglyphic): {X.shape}\")\n",
                "print(f\"  Y (English): {Y.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Compute Procrustes Transformation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Computing Procrustes transformation...\n",
                        "âœ“ Transformation matrix computed\n",
                        "  Shape: (300, 300)\n",
                        "  Scale factor: 80381.3102\n"
                    ]
                }
            ],
            "source": [
                "print(\"Computing Procrustes transformation...\")\n",
                "R, scale = orthogonal_procrustes(X, Y)\n",
                "\n",
                "print(f\"âœ“ Transformation matrix computed\")\n",
                "print(f\"  Shape: {R.shape}\")\n",
                "print(f\"  Scale factor: {scale:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Align Hieroglyphic Space (Vectorized)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Aligning hieroglyphic vectors...\n",
                        "âœ“ Aligned 11,974 vectors\n",
                        "  Aligned matrix shape: (11974, 300)\n"
                    ]
                }
            ],
            "source": [
                "# Create aligned hieroglyphic matrix (vectorized!)\n",
                "print(\"Aligning hieroglyphic vectors...\")\n",
                "\n",
                "# Get all hieroglyphic vectors as a matrix\n",
                "hier_words = hier_wv.index_to_key\n",
                "hier_matrix = np.array([hier_wv[w] for w in hier_words])\n",
                "\n",
                "# Transform all at once: aligned = hier_matrix @ R\n",
                "aligned_matrix = hier_matrix @ R\n",
                "\n",
                "print(f\"âœ“ Aligned {len(hier_words):,} vectors\")\n",
                "print(f\"  Aligned matrix shape: {aligned_matrix.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Optimized Translation Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ“ Optimized translation function ready\n"
                    ]
                }
            ],
            "source": [
                "def translate_batch(h_indices, topn=5):\n",
                "    \"\"\"\n",
                "    Translate multiple hieroglyphic words at once (vectorized).\n",
                "    h_indices: list of indices into hier_words\n",
                "    \"\"\"\n",
                "    # Get aligned vectors for these words\n",
                "    h_vecs = aligned_matrix[h_indices]\n",
                "    \n",
                "    # Compute cosine similarity with all English words (vectorized!)\n",
                "    # Shape: (len(h_indices), len(eng_words))\n",
                "    similarities = cosine_similarity(h_vecs, eng_matrix)\n",
                "    \n",
                "    # Get top N for each word\n",
                "    results = []\n",
                "    for i, sim_row in enumerate(similarities):\n",
                "        top_indices = np.argsort(sim_row)[-topn:][::-1]\n",
                "        top_words = [(eng_words[idx], sim_row[idx]) for idx in top_indices]\n",
                "        results.append(top_words)\n",
                "    \n",
                "    return results\n",
                "\n",
                "# Create index lookup\n",
                "hier_word_to_idx = {word: i for i, word in enumerate(hier_words)}\n",
                "\n",
                "print(\"âœ“ Optimized translation function ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Test Translations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Translation Tests:\n",
                        "======================================================================\n",
                        "\n",
                        "âœ“ wsjr            (expected: osiris)\n",
                        "    osiris               (score: 0.615)\n",
                        "    der                  (score: 0.404)\n",
                        "    anubis               (score: 0.387)\n",
                        "    isis                 (score: 0.324)\n",
                        "    und                  (score: 0.321)\n",
                        "\n",
                        "âœ“ á¸¥r,w            (expected: horus)\n",
                        "    horus                (score: 0.621)\n",
                        "    der                  (score: 0.402)\n",
                        "    zum                  (score: 0.346)\n",
                        "    anubis               (score: 0.343)\n",
                        "    deutschen            (score: 0.339)\n",
                        "\n",
                        "âœ“ ppy             (expected: pepi)\n",
                        "    pepi                 (score: 0.671)\n",
                        "    ist                  (score: 0.390)\n",
                        "    gott                 (score: 0.387)\n",
                        "    der                  (score: 0.353)\n",
                        "    auf                  (score: 0.351)\n",
                        "\n",
                        "âœ“ zêœ£              (expected: son)\n",
                        "    son                  (score: 0.474)\n",
                        "    father               (score: 0.444)\n",
                        "    der                  (score: 0.419)\n",
                        "    eldest               (score: 0.407)\n",
                        "    grandfather          (score: 0.388)\n",
                        "\n",
                        "âœ“ ná¹¯r             (expected: god)\n",
                        "    god                  (score: 0.613)\n",
                        "    christ               (score: 0.415)\n",
                        "    scripture            (score: 0.412)\n",
                        "    divine               (score: 0.407)\n",
                        "    bless                (score: 0.407)\n",
                        "\n",
                        "âœ“ mw              (expected: water)\n",
                        "    water                (score: 0.577)\n",
                        "    potable              (score: 0.455)\n",
                        "    irrigation           (score: 0.405)\n",
                        "    groundwater          (score: 0.398)\n",
                        "    seawater             (score: 0.381)\n",
                        "\n",
                        "âœ— êœ¥ná¸«             (expected: life)\n",
                        "    live                 (score: 0.474)\n",
                        "    im                   (score: 0.391)\n",
                        "    und                  (score: 0.378)\n",
                        "    der                  (score: 0.349)\n",
                        "    premiered            (score: 0.349)\n"
                    ]
                }
            ],
            "source": [
                "# Test words\n",
                "test_cases = [\n",
                "    ('wsjr', 'osiris'),\n",
                "    ('á¸¥r,w', 'horus'),\n",
                "    ('ppy', 'pepi'),\n",
                "    ('zêœ£', 'son'),\n",
                "    ('ná¹¯r', 'god'),\n",
                "    ('mw', 'water'),\n",
                "    ('êœ¥ná¸«', 'life'),\n",
                "]\n",
                "\n",
                "# Get indices for test words\n",
                "test_indices = [hier_word_to_idx[w] for w, _ in test_cases if w in hier_word_to_idx]\n",
                "test_words_found = [w for w, _ in test_cases if w in hier_word_to_idx]\n",
                "\n",
                "# Translate all at once!\n",
                "results = translate_batch(test_indices, topn=5)\n",
                "\n",
                "print(\"Translation Tests:\")\n",
                "print(\"=\"*70)\n",
                "for (h_word, expected), predictions in zip([(w, e) for w, e in test_cases if w in hier_word_to_idx], results):\n",
                "    top_word, top_score = predictions[0]\n",
                "    match = \"âœ“\" if top_word.lower() == expected.lower() else \"âœ—\"\n",
                "    \n",
                "    print(f\"\\n{match} {h_word:15s} (expected: {expected})\")\n",
                "    for word, score in predictions:\n",
                "        print(f\"    {word:20s} (score: {score:.3f})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Evaluate on Anchors (Fast!)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Evaluating anchors (vectorized)...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Batches:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [01:41<02:47, 33.53s/it]"
                    ]
                }
            ],
            "source": [
                "# Batch evaluation\n",
                "print(\"Evaluating anchors (vectorized)...\")\n",
                "\n",
                "# Get indices for all valid anchors\n",
                "anchor_h_indices = [hier_word_to_idx[a['hieroglyphic']] for a in valid_anchors]\n",
                "anchor_e_words = [a['english'] for a in valid_anchors]\n",
                "\n",
                "# Translate all anchors at once (FAST!)\n",
                "BATCH_SIZE = 1000\n",
                "all_predictions = []\n",
                "\n",
                "for i in tqdm(range(0, len(anchor_h_indices), BATCH_SIZE), desc=\"Batches\"):\n",
                "    batch_indices = anchor_h_indices[i:i+BATCH_SIZE]\n",
                "    batch_results = translate_batch(batch_indices, topn=5)\n",
                "    all_predictions.extend(batch_results)\n",
                "\n",
                "# Calculate accuracy\n",
                "correct = 0\n",
                "top5_correct = 0\n",
                "\n",
                "for expected, predictions in zip(anchor_e_words, all_predictions):\n",
                "    top_word = predictions[0][0]\n",
                "    \n",
                "    if top_word.lower() == expected.lower():\n",
                "        correct += 1\n",
                "    \n",
                "    if expected.lower() in [w.lower() for w, _ in predictions]:\n",
                "        top5_correct += 1\n",
                "\n",
                "total = len(valid_anchors)\n",
                "accuracy = correct / total * 100\n",
                "top5_accuracy = top5_correct / total * 100\n",
                "\n",
                "print(\"\\nEvaluation Results:\")\n",
                "print(\"=\"*70)\n",
                "print(f\"Total anchors: {total:,}\")\n",
                "print(f\"\\nTop-1 Accuracy: {correct:,} / {total:,} = {accuracy:.2f}%\")\n",
                "print(f\"Top-5 Accuracy: {top5_correct:,} / {total:,} = {top5_accuracy:.2f}%\")\n",
                "print(f\"\\nV3 Baseline: 22.0%\")\n",
                "print(f\"V5 Improvement: {accuracy - 22:+.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Discover New Meanings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Interesting words\n",
                "discovery_words = ['inpw', 'wsjr', 'á¸¥r,w', 'ná¹¯r', 'á¸¥qt', 'rêœ¥w']\n",
                "discovery_indices = [hier_word_to_idx[w] for w in discovery_words if w in hier_word_to_idx]\n",
                "discovery_found = [w for w in discovery_words if w in hier_word_to_idx]\n",
                "\n",
                "results = translate_batch(discovery_indices, topn=10)\n",
                "\n",
                "print(\"New Discoveries:\")\n",
                "print(\"=\"*70)\n",
                "for word, predictions in zip(discovery_found, results):\n",
                "    print(f\"\\n{word}:\")\n",
                "    for i, (e_word, score) in enumerate(predictions, 1):\n",
                "        print(f\"  {i:2d}. {e_word:20s} (score: {score:.3f})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Save Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save transformation matrix\n",
                "np.save('../data/processed/procrustes_matrix.npy', R)\n",
                "\n",
                "# Save results\n",
                "results = {\n",
                "    'total_anchors': total,\n",
                "    'top1_correct': int(correct),\n",
                "    'top1_accuracy': float(accuracy),\n",
                "    'top5_correct': int(top5_correct),\n",
                "    'top5_accuracy': float(top5_accuracy),\n",
                "    'v3_baseline': 22.0,\n",
                "    'improvement': float(accuracy - 22.0)\n",
                "}\n",
                "\n",
                "with open('../data/processed/alignment_results.json', 'w') as f:\n",
                "    json.dump(results, f, indent=2)\n",
                "\n",
                "print(\"âœ“ Saved results\")\n",
                "print(f\"\\nðŸŽ‰ Final V5 Accuracy: {accuracy:.2f}%\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python (spontaneous-remission)",
            "language": "python",
            "name": "spontaneous-remission"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
