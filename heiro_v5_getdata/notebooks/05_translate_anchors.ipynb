{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 2: Translate German Anchors to English\n",
                "\n",
                "## Goal\n",
                "Translate the 8,541 German anchor words to English using DeepL API.\n",
                "This creates the hieroglyphic ↔ English anchor pairs needed for alignment.\n",
                "\n",
                "## Strategy\n",
                "1. Load `german_anchors.json`\n",
                "2. Extract unique German words\n",
                "3. Batch translate using DeepL API (most accurate for German→English)\n",
                "4. Create final anchor dictionary: hieroglyphic → English\n",
                "5. Save for embedding alignment\n",
                "\n",
                "## Cost Estimate\n",
                "- DeepL Free: 500k characters/month\n",
                "- Our ~8.5k words ≈ 85k characters\n",
                "- **Cost: FREE** (within free tier)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded 8,541 anchors\n",
                        "\n",
                        "Sample:\n",
                        "  n               → der                  (conf: 0.34)\n",
                        "  m               → der                  (conf: 0.37)\n",
                        "  =f              → er                   (conf: 0.38)\n",
                        "  =k              → du                   (conf: 0.45)\n",
                        "  =j              → ich                  (conf: 0.60)\n"
                    ]
                }
            ],
            "source": [
                "import json\n",
                "import pickle\n",
                "from pathlib import Path\n",
                "from collections import Counter\n",
                "import time\n",
                "\n",
                "# Load German anchors\n",
                "anchors_path = Path('../data/processed/german_anchors.json')\n",
                "with open(anchors_path, 'r', encoding='utf-8') as f:\n",
                "    german_anchors = json.load(f)\n",
                "\n",
                "print(f\"Loaded {len(german_anchors):,} anchors\")\n",
                "print(f\"\\nSample:\")\n",
                "for i in range(5):\n",
                "    a = german_anchors[i]\n",
                "    print(f\"  {a['hieroglyphic']:15s} → {a['german']:20s} (conf: {a['confidence']:.2f})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Unique German words to translate: 1,917\n",
                        "Total characters: 13,212\n",
                        "\n",
                        "Within DeepL free tier: True\n"
                    ]
                }
            ],
            "source": [
                "# Extract unique German words\n",
                "german_words = list(set([a['german'] for a in german_anchors]))\n",
                "print(f\"Unique German words to translate: {len(german_words):,}\")\n",
                "\n",
                "# Estimate character count\n",
                "total_chars = sum(len(w) for w in german_words)\n",
                "print(f\"Total characters: {total_chars:,}\")\n",
                "print(f\"\\nWithin DeepL free tier: {total_chars < 500000}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## DeepL API Setup\n",
                "\n",
                "To use DeepL:\n",
                "1. Sign up at https://www.deepl.com/pro-api\n",
                "2. Get your API key (free tier: 500k chars/month)\n",
                "3. Install: `pip install deepl`\n",
                "4. Set environment variable: `export DEEPL_API_KEY=\"your-key\"`\n",
                "\n",
                "**Alternative:** Use Google Translate (less accurate for German)\n",
                "```python\n",
                "from googletrans import Translator\n",
                "translator = Translator()\n",
                "result = translator.translate(text, src='de', dest='en')\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "⚠️  DEEPL_API_KEY not found in environment\n",
                        "Please set it: export DEEPL_API_KEY='your-key'\n",
                        "DeepL not available: API key required\n",
                        "\n",
                        "Falling back to Google Translate...\n",
                        "❌ No translation library available\n",
                        "Install one: pip install deepl OR pip install googletrans==4.0.0-rc1\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "# Option 1: DeepL (recommended)\n",
                "try:\n",
                "    import deepl\n",
                "    import os\n",
                "    \n",
                "    api_key = os.getenv('DEEPL_API_KEY')\n",
                "    if not api_key:\n",
                "        print(\"⚠️  DEEPL_API_KEY not found in environment\")\n",
                "        print(\"Please set it: export DEEPL_API_KEY='your-key'\")\n",
                "        raise ImportError(\"API key required\")\n",
                "    \n",
                "    translator = deepl.Translator(api_key)\n",
                "    print(\"✓ DeepL translator initialized\")\n",
                "    \n",
                "    # Test translation\n",
                "    test = translator.translate_text(\"Gott\", target_lang=\"EN-US\")\n",
                "    print(f\"Test: 'Gott' → '{test.text}'\")\n",
                "    \n",
                "except ImportError as e:\n",
                "    print(f\"DeepL not available: {e}\")\n",
                "    print(\"\\nFalling back to Google Translate...\")\n",
                "    \n",
                "    try:\n",
                "        from googletrans import Translator\n",
                "        translator = Translator()\n",
                "        print(\"✓ Google Translate initialized\")\n",
                "        \n",
                "        # Test translation\n",
                "        test = translator.translate(\"Gott\", src='de', dest='en')\n",
                "        print(f\"Test: 'Gott' → '{test.text}'\")\n",
                "        \n",
                "    except ImportError:\n",
                "        print(\"❌ No translation library available\")\n",
                "        print(\"Install one: pip install deepl OR pip install googletrans==4.0.0-rc1\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Translating 1,917 words using DeepL...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Batches: 100%|██████████| 39/39 [00:24<00:00,  1.61it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Translated 1,917 / 1,917 words\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Translate in batches\n",
                "from tqdm import tqdm\n",
                "\n",
                "translations = {}\n",
                "BATCH_SIZE = 50  # Translate 50 words at a time\n",
                "USE_DEEPL = 'deepl' in str(type(translator)).lower()\n",
                "\n",
                "print(f\"Translating {len(german_words):,} words using {'DeepL' if USE_DEEPL else 'Google Translate'}...\")\n",
                "\n",
                "for i in tqdm(range(0, len(german_words), BATCH_SIZE), desc=\"Batches\"):\n",
                "    batch = german_words[i:i+BATCH_SIZE]\n",
                "    \n",
                "    try:\n",
                "        if USE_DEEPL:\n",
                "            # DeepL batch translation\n",
                "            results = translator.translate_text(batch, target_lang=\"EN-US\")\n",
                "            for german, result in zip(batch, results):\n",
                "                translations[german] = result.text.lower()\n",
                "        else:\n",
                "            # Google Translate (one by one)\n",
                "            for german in batch:\n",
                "                result = translator.translate(german, src='de', dest='en')\n",
                "                translations[german] = result.text.lower()\n",
                "                time.sleep(0.1)  # Rate limiting\n",
                "    except Exception as e:\n",
                "        print(f\"\\nError in batch {i//BATCH_SIZE}: {e}\")\n",
                "        # Continue with next batch\n",
                "        continue\n",
                "\n",
                "print(f\"\\nTranslated {len(translations):,} / {len(german_words):,} words\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Sample Translations:\n",
                        "==================================================\n",
                        " 1. festduftöl           → solid fragrance oil\n",
                        " 2. taub                 → deaf\n",
                        " 3. himmelskuh           → heavenly cow\n",
                        " 4. nahen                → near\n",
                        " 5. mnw                  → mnw\n",
                        " 6. 25                   → 25\n",
                        " 7. entstanden           → originated\n",
                        " 8. schwellung           → swelling\n",
                        " 9. rptrf                → rptrf\n",
                        "10. hymne                → hymn\n",
                        "11. 34                   → 34\n",
                        "12. bien                 → well\n",
                        "13. amme                 → amme\n",
                        "14. salz                 → salt\n",
                        "15. nwmmwt               → nwmmwt\n",
                        "16. meketaton            → meketaton\n",
                        "17. speichel             → saliva\n",
                        "18. ville                → city\n",
                        "19. fähre                → ferry\n",
                        "20. palastes             → palaces\n"
                    ]
                }
            ],
            "source": [
                "# Show sample translations\n",
                "print(\"Sample Translations:\")\n",
                "print(\"=\"*50)\n",
                "for i, (de, en) in enumerate(list(translations.items())[:20], 1):\n",
                "    print(f\"{i:2d}. {de:20s} → {en}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Created 8,541 English anchors\n",
                        "\n",
                        "Top 10:\n",
                        " 1. n               → der                  (was: der)\n",
                        " 2. m               → der                  (was: der)\n",
                        " 3. =f              → er                   (was: er)\n",
                        " 4. =k              → du                   (was: du)\n",
                        " 5. =j              → ich                  (was: ich)\n",
                        " 6. ḥr              → der                  (was: der)\n",
                        " 7. r               → der                  (was: der)\n",
                        " 8. wsjr            → osiris               (was: osiris)\n",
                        " 9. =sn             → sie                  (was: sie)\n",
                        "10. pw              → ist                  (was: ist)\n"
                    ]
                }
            ],
            "source": [
                "# Create English anchors\n",
                "english_anchors = []\n",
                "\n",
                "for anchor in german_anchors:\n",
                "    german = anchor['german']\n",
                "    if german in translations:\n",
                "        english_anchors.append({\n",
                "            'hieroglyphic': anchor['hieroglyphic'],\n",
                "            'english': translations[german],\n",
                "            'german': german,  # Keep for reference\n",
                "            'confidence': anchor['confidence'],\n",
                "            'frequency': anchor['frequency']\n",
                "        })\n",
                "\n",
                "print(f\"Created {len(english_anchors):,} English anchors\")\n",
                "print(f\"\\nTop 10:\")\n",
                "for i, a in enumerate(english_anchors[:10], 1):\n",
                "    print(f\"{i:2d}. {a['hieroglyphic']:15s} → {a['english']:20s} (was: {a['german']})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Saved 8,541 English anchors to:\n",
                        "  - ../data/processed/english_anchors.pkl\n",
                        "  - ../data/processed/english_anchors.json\n"
                    ]
                }
            ],
            "source": [
                "# Save English anchors\n",
                "output_pkl = Path('../data/processed/english_anchors.pkl')\n",
                "output_json = Path('../data/processed/english_anchors.json')\n",
                "\n",
                "with open(output_pkl, 'wb') as f:\n",
                "    pickle.dump(english_anchors, f)\n",
                "\n",
                "with open(output_json, 'w', encoding='utf-8') as f:\n",
                "    json.dump(english_anchors, f, ensure_ascii=False, indent=2)\n",
                "\n",
                "print(f\"\\nSaved {len(english_anchors):,} English anchors to:\")\n",
                "print(f\"  - {output_pkl}\")\n",
                "print(f\"  - {output_json}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Comparison with V3\n",
                "\n",
                "Let's compare our V5 anchors with V3's successful ones."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "V3 vs V5 Anchor Comparison:\n",
                        "======================================================================\n",
                        "✗ =f              | V3: the             | V5: er\n",
                        "✗ =k              | V3: you             | V5: du\n",
                        "✗ m               | V3: the             | V5: der\n",
                        "✗ n               | V3: the             | V5: der\n",
                        "✗ ḥr.w            | V3: horus           | V5: NOT FOUND\n",
                        "✗ wnḏs            | V3: unas            | V5: NOT FOUND\n",
                        "✗ pn              | V3: this            | V5: der\n",
                        "✓ zꜣ              | V3: son             | V5: son\n",
                        "✗ pw              | V3: the             | V5: ist\n",
                        "✗ ḏd-mdw          | V3: words           | V5: zu\n",
                        "✓ ppy             | V3: pepi            | V5: pepi\n",
                        "✗ wsꜣr            | V3: osiris          | V5: NOT FOUND\n",
                        "✗ nꜣ.t            | V3: neith           | V5: NOT FOUND\n",
                        "✗ wsr.w           | V3: osiris          | V5: der\n"
                    ]
                }
            ],
            "source": [
                "# V3's top anchors (from the notebook we saw earlier)\n",
                "v3_anchors = {\n",
                "    '=f': 'the',\n",
                "    '=k': 'you',\n",
                "    'm': 'the',\n",
                "    'n': 'the',\n",
                "    'ḥr.w': 'horus',\n",
                "    'wnḏs': 'unas',\n",
                "    'pn': 'this',\n",
                "    'zꜣ': 'son',\n",
                "    'pw': 'the',\n",
                "    'ḏd-mdw': 'words',\n",
                "    'ppy': 'pepi',\n",
                "    'wsꜣr': 'osiris',\n",
                "    'nꜣ.t': 'neith',\n",
                "    'wsr.w': 'osiris'\n",
                "}\n",
                "\n",
                "# Check overlap\n",
                "v5_dict = {a['hieroglyphic']: a['english'] for a in english_anchors}\n",
                "\n",
                "print(\"V3 vs V5 Anchor Comparison:\")\n",
                "print(\"=\"*70)\n",
                "for h, v3_en in v3_anchors.items():\n",
                "    v5_en = v5_dict.get(h, \"NOT FOUND\")\n",
                "    match = \"✓\" if v5_en == v3_en else \"✗\"\n",
                "    print(f\"{match} {h:15s} | V3: {v3_en:15s} | V5: {v5_en}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Next Steps: Phase 3 - Embedding Training\n",
                "\n",
                "Now that we have English anchors, we can:\n",
                "1. Train FastText on hieroglyphic corpus (104k texts)\n",
                "2. Load/train English embeddings (Wikipedia)\n",
                "3. Align using Orthogonal Procrustes\n",
                "4. Evaluate against V3's test set\n",
                "\n",
                "Create `06_embedding_training.ipynb` next."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python (spontaneous-remission)",
            "language": "python",
            "name": "spontaneous-remission"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
