{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook: CSLS Alignment (Attempt 4)\n",
    "\n",
    "## Goal\n",
    "In Attempt 3, we used **Procrustes Analysis** to align the spaces and **Nearest Neighbors (NN)** to translate.\n",
    "However, NN suffers from the **Hubness Problem**: some words (like \"the\", \"is\") appear as neighbors to *everything* just because they are in a dense part of the vector space.\n",
    "\n",
    "**CSLS (Cross-Domain Similarity Local Scaling)** fixes this. It penalizes words that are \"hubs\".\n",
    "Instead of just asking \"Are you close to me?\", it asks \"Are you close to me AND not close to everyone else?\"\n",
    "\n",
    "## Steps\n",
    "1.  Load the pre-trained models from V3.\n",
    "2.  Calculate the Procrustes Rotation Matrix $R$ (same as V3).\n",
    "3.  Implement the **CSLS** metric.\n",
    "4.  Compare CSLS vs. Standard NN accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T17:26:02.254596Z",
     "iopub.status.busy": "2025-11-19T17:26:02.254442Z",
     "iopub.status.idle": "2025-11-19T17:26:03.764969Z",
     "shell.execute_reply": "2025-11-19T17:26:03.764142Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading anchors...\n",
      "Loaded 1362 anchors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from gensim.models import FastText, Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = \"data\"\n",
    "MODELS_DIR = \"models\"\n",
    "ANCHOR_FILE = os.path.join(DATA_DIR, \"anchors.pkl\")\n",
    "HIEROGLYPHIC_MODEL_FILE = os.path.join(MODELS_DIR, \"hieroglyphic_fasttext.model\")\n",
    "ENGLISH_MODEL_FILE = os.path.join(MODELS_DIR, \"english_word2vec.model\")\n",
    "\n",
    "print(\"Loading models...\")\n",
    "hier_model = FastText.load(HIEROGLYPHIC_MODEL_FILE)\n",
    "eng_model = Word2Vec.load(ENGLISH_MODEL_FILE)\n",
    "\n",
    "print(\"Loading anchors...\")\n",
    "with open(ANCHOR_FILE, 'rb') as f:\n",
    "    anchors = pickle.load(f)\n",
    "    \n",
    "print(f\"Loaded {len(anchors)} anchors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Re-Calculate Rotation (Procrustes)\n",
    "We need to re-do the alignment step to get our rotation matrix $R$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T17:26:03.767103Z",
     "iopub.status.busy": "2025-11-19T17:26:03.766897Z",
     "iopub.status.idle": "2025-11-19T17:26:03.791096Z",
     "shell.execute_reply": "2025-11-19T17:26:03.785109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rotation matrix R calculated.\n"
     ]
    }
   ],
   "source": [
    "# Prepare matrices\n",
    "valid_anchors = []\n",
    "X_list = []\n",
    "Y_list = []\n",
    "\n",
    "for anchor in anchors:\n",
    "    h_word = anchor['hieroglyphic']\n",
    "    e_word = anchor['english']\n",
    "    if e_word in eng_model.wv:\n",
    "        valid_anchors.append((h_word, e_word))\n",
    "        X_list.append(hier_model.wv[h_word])\n",
    "        Y_list.append(eng_model.wv[e_word])\n",
    "\n",
    "X = np.array(X_list)\n",
    "Y = np.array(Y_list)\n",
    "\n",
    "# Split\n",
    "X_train, X_test, Y_train, Y_test, anchors_train, anchors_test = train_test_split(\n",
    "    X, Y, valid_anchors, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# SVD for Rotation\n",
    "U, S, Vt = np.linalg.svd(Y_train.T @ X_train)\n",
    "R = U @ Vt\n",
    "\n",
    "print(\"Rotation matrix R calculated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement CSLS\n",
    "\n",
    "CSLS is defined as:\n",
    "$$ CSLS(x, y) = 2 \\cos(x, y) - r_T(x) - r_S(y) $$\n",
    "Where $r_T(x)$ is the average similarity of $x$ to its $k$ nearest neighbors in the target space.\n",
    "\n",
    "Basically: \"Similarity minus Popularity\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T17:26:03.801792Z",
     "iopub.status.busy": "2025-11-19T17:26:03.801447Z",
     "iopub.status.idle": "2025-11-19T17:26:03.826808Z",
     "shell.execute_reply": "2025-11-19T17:26:03.818597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target space prepared: 4177 words.\n"
     ]
    }
   ],
   "source": [
    "def get_csls_scores(source_vecs, target_vecs, k=10):\n",
    "    \"\"\"\n",
    "    Compute CSLS scores between source and target vectors.\n",
    "    source_vecs: (N, dim) - Projected hieroglyphic vectors\n",
    "    target_vecs: (M, dim) - All English vectors\n",
    "    \"\"\"\n",
    "    # Normalize vectors for cosine similarity\n",
    "    source_norm = source_vecs / np.linalg.norm(source_vecs, axis=1, keepdims=True)\n",
    "    target_norm = target_vecs / np.linalg.norm(target_vecs, axis=1, keepdims=True)\n",
    "    \n",
    "    # Compute Cosine Similarity Matrix (N x M)\n",
    "    # This can be large, so be careful with memory in production\n",
    "    sim_matrix = np.dot(source_norm, target_norm.T)\n",
    "    \n",
    "    # Calculate r_T (average sim to k nearest neighbors in target for each source)\n",
    "    # For each row (source word), find top k in target\n",
    "    r_T = np.mean(np.sort(sim_matrix, axis=1)[:, -k:], axis=1)\n",
    "    \n",
    "    # Calculate r_S (average sim to k nearest neighbors in source for each target)\n",
    "    # For each col (target word), find top k in source\n",
    "    r_S = np.mean(np.sort(sim_matrix, axis=0)[-k:, :], axis=0)\n",
    "    \n",
    "    # CSLS = 2*cos - r_T - r_S\n",
    "    # Broadcast r_T (N, 1) and r_S (1, M)\n",
    "    csls_scores = 2 * sim_matrix - r_T[:, np.newaxis] - r_S[np.newaxis, :]\n",
    "    \n",
    "    return csls_scores\n",
    "\n",
    "# Prepare Target Space (All English Words)\n",
    "# We'll limit to top 20k words to keep it fast\n",
    "target_words = eng_model.wv.index_to_key[:20000]\n",
    "target_vecs = np.array([eng_model.wv[w] for w in target_words])\n",
    "\n",
    "print(f\"Target space prepared: {len(target_words)} words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate CSLS vs NN\n",
    "\n",
    "Let's compare the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T17:26:03.835591Z",
     "iopub.status.busy": "2025-11-19T17:26:03.835245Z",
     "iopub.status.idle": "2025-11-19T17:26:04.207042Z",
     "shell.execute_reply": "2025-11-19T17:26:04.203473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating CSLS...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSLS Top-1 Accuracy: 5.86%\n",
      "CSLS Top-10 Accuracy: 15.38%\n",
      "\n",
      "(Recall V3 NN Top-10 was ~22%)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_csls(X_test, anchors_test, R, top_k=10):\n",
    "    # Project Test Vectors\n",
    "    X_projected = X_test @ R.T\n",
    "    \n",
    "    # Compute CSLS Matrix (Test_Size x Target_Vocab_Size)\n",
    "    scores = get_csls_scores(X_projected, target_vecs)\n",
    "    \n",
    "    hits = 0\n",
    "    total = len(anchors_test)\n",
    "    \n",
    "    for i, (h_word, true_e_word) in enumerate(anchors_test):\n",
    "        # Get top k indices for this word\n",
    "        top_indices = np.argsort(scores[i])[-top_k:][::-1]\n",
    "        candidates = [target_words[idx] for idx in top_indices]\n",
    "        \n",
    "        if true_e_word in candidates:\n",
    "            hits += 1\n",
    "            \n",
    "    return hits / total\n",
    "\n",
    "print(\"Evaluating CSLS...\")\n",
    "csls_acc_1 = evaluate_csls(X_test, anchors_test, R, top_k=1)\n",
    "csls_acc_10 = evaluate_csls(X_test, anchors_test, R, top_k=10)\n",
    "\n",
    "print(f\"CSLS Top-1 Accuracy: {csls_acc_1:.2%}\")\n",
    "print(f\"CSLS Top-10 Accuracy: {csls_acc_10:.2%}\")\n",
    "\n",
    "# Compare with V3 (NN) results from memory/previous run\n",
    "print(\"\\n(Recall V3 NN Top-10 was ~22%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Translation Demo\n",
    "\n",
    "Let's see if the translations make more sense now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T17:26:04.214528Z",
     "iopub.status.busy": "2025-11-19T17:26:04.214217Z",
     "iopub.status.idle": "2025-11-19T17:26:04.238211Z",
     "shell.execute_reply": "2025-11-19T17:26:04.234318Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CSLS Translation for 'nfr':\n",
      "  -> wemetetka (0.055)\n",
      "  -> tjetu (0.028)\n",
      "  -> worried (0.008)\n",
      "  -> behdeti (0.007)\n",
      "  -> hetepeniptah (-0.002)\n",
      "\n",
      "CSLS Translation for 'pr-aa':\n",
      "  -> sacrifice (0.031)\n",
      "  -> thotfest (0.030)\n",
      "  -> writing (0.025)\n",
      "  -> kapriests (0.022)\n",
      "  -> subjects (0.010)\n",
      "\n",
      "CSLS Translation for 'ra':\n",
      "  -> demedj (0.080)\n",
      "  -> nisachmetanch (0.040)\n",
      "  -> nose (0.002)\n",
      "  -> inside (0.000)\n",
      "  -> tjauti (-0.008)\n"
     ]
    }
   ],
   "source": [
    "def translate_csls(h_word, top_k=5):\n",
    "    if h_word not in hier_model.wv:\n",
    "        print(f\"'{h_word}' not in vocab.\")\n",
    "        return\n",
    "        \n",
    "    vec = hier_model.wv[h_word]\n",
    "    proj_vec = (vec @ R.T).reshape(1, -1)\n",
    "    \n",
    "    scores = get_csls_scores(proj_vec, target_vecs)\n",
    "    top_indices = np.argsort(scores[0])[-top_k:][::-1]\n",
    "    \n",
    "    print(f\"\\nCSLS Translation for '{h_word}':\")\n",
    "    for idx in top_indices:\n",
    "        print(f\"  -> {target_words[idx]} ({scores[0][idx]:.3f})\")\n",
    "\n",
    "translate_csls(\"nfr\")\n",
    "translate_csls(\"pr-aa\")\n",
    "translate_csls(\"ra\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heiroglyphy",
   "language": "python",
   "name": "heiroglyphy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}