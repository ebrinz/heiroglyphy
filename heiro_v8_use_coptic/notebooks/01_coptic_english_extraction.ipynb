{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# V8 Data Collection: Coptic-English Bible Corpus\n",
        "\n",
        "## Goal\n",
        "Extract Coptic-English parallel verses from OPUS Bible corpus to create mappings for anchor enhancement.\n",
        "\n",
        "## Data Source\n",
        "- **OPUS Bible corpus**: Coptic-English TMX file\n",
        "- **Format**: Translation Memory eXchange (TMX)\n",
        "- **Size**: ~1.5MB compressed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Download\n",
        "\n",
        "Download the OPUS Coptic-English Bible corpus (TMX format)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "# Ensure data directory exists\n",
        "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
        "RAW_DATA_DIR = PROJECT_ROOT / \"data/raw\"\n",
        "RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "TMX_URL = \"https://object.pouta.csc.fi/OPUS-bible-uedin/v1/tmx/cop-en.tmx.gz\"\n",
        "TMX_PATH = RAW_DATA_DIR / \"cop-en.tmx.gz\"\n",
        "\n",
        "# Download if not already present\n",
        "if not TMX_PATH.exists():\n",
        "    print(f\"Downloading Coptic-English Bible corpus...\")\n",
        "    result = subprocess.run(\n",
        "        [\"curl\", \"-L\", TMX_URL, \"-o\", str(TMX_PATH)],\n",
        "        capture_output=True,\n",
        "        text=True\n",
        "    )\n",
        "    if result.returncode == 0:\n",
        "        print(f\"\u2713 Downloaded to {TMX_PATH}\")\n",
        "        print(f\"  Size: {TMX_PATH.stat().st_size / 1024:.1f} KB\")\n",
        "    else:\n",
        "        print(f\"\u2717 Download failed: {result.stderr}\")\n",
        "else:\n",
        "    print(f\"\u2713 File already exists: {TMX_PATH}\")\n",
        "    print(f\"  Size: {TMX_PATH.stat().st_size / 1024:.1f} KB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gzip\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "import json\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "# Paths\n",
        "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
        "TMX_PATH = PROJECT_ROOT / 'data/raw/cop-en.tmx.gz'\n",
        "OUTPUT_PATH = PROJECT_ROOT / 'data/processed/coptic_english_bible.json'\n",
        "\n",
        "print(f'Project root: {PROJECT_ROOT}')\n",
        "print(f'TMX file: {TMX_PATH}')\n",
        "print(f'TMX exists: {TMX_PATH.exists()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parse TMX File\n",
        "\n",
        "TMX format structure:\n",
        "```xml\n",
        "<tu>\n",
        "  <tuv xml:lang=\"cop\"><seg>Coptic text</seg></tuv>\n",
        "  <tuv xml:lang=\"en\"><seg>English text</seg></tuv>\n",
        "</tu>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_tmx(tmx_path):\n",
        "    \"\"\"Parse TMX file and extract Coptic-English parallel verses.\"\"\"\n",
        "    parallel_verses = []\n",
        "    \n",
        "    with gzip.open(tmx_path, 'rt', encoding='utf-8') as f:\n",
        "        tree = ET.parse(f)\n",
        "        root = tree.getroot()\n",
        "        \n",
        "        # Find all translation units\n",
        "        for tu in root.findall('.//tu'):\n",
        "            coptic_text = None\n",
        "            english_text = None\n",
        "            \n",
        "            # Extract Coptic and English segments\n",
        "            for tuv in tu.findall('tuv'):\n",
        "                lang = tuv.get('{http://www.w3.org/XML/1998/namespace}lang')\n",
        "                seg = tuv.find('seg')\n",
        "                \n",
        "                if seg is not None and seg.text:\n",
        "                    if lang == 'cop':\n",
        "                        coptic_text = seg.text.strip()\n",
        "                    elif lang == 'en':\n",
        "                        english_text = seg.text.strip()\n",
        "            \n",
        "            # Add to parallel verses if both exist\n",
        "            if coptic_text and english_text:\n",
        "                parallel_verses.append({\n",
        "                    'coptic': coptic_text,\n",
        "                    'english': english_text\n",
        "                })\n",
        "    \n",
        "    return parallel_verses\n",
        "\n",
        "print('Parsing TMX file...')\n",
        "parallel_verses = parse_tmx(TMX_PATH)\n",
        "print(f'Extracted {len(parallel_verses)} parallel verses')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspect Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show first 10 examples\n",
        "print('First 10 Coptic-English verse pairs:\\n')\n",
        "for i, verse in enumerate(parallel_verses[:10]):\n",
        "    print(f\"{i+1}. Coptic:  {verse['coptic']}\")\n",
        "    print(f\"   English: {verse['english']}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistics\n",
        "coptic_words = [word for verse in parallel_verses for word in verse['coptic'].split()]\n",
        "english_words = [word for verse in parallel_verses for word in verse['english'].split()]\n",
        "\n",
        "print(f'Total verses: {len(parallel_verses)}')\n",
        "print(f'Total Coptic words: {len(coptic_words)}')\n",
        "print(f'Unique Coptic words: {len(set(coptic_words))}')\n",
        "print(f'Total English words: {len(english_words)}')\n",
        "print(f'Unique English words: {len(set(english_words))}')\n",
        "print(f'Avg Coptic words/verse: {len(coptic_words) / len(parallel_verses):.1f}')\n",
        "print(f'Avg English words/verse: {len(english_words) / len(parallel_verses):.1f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Word-Level Alignments\n",
        "\n",
        "Extract common Coptic-English word pairs using co-occurrence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_word_cooccurrences(parallel_verses, min_count=3):\n",
        "    \"\"\"Extract Coptic-English word pairs based on co-occurrence.\"\"\"\n",
        "    cooccurrence = defaultdict(lambda: defaultdict(int))\n",
        "    \n",
        "    for verse in parallel_verses:\n",
        "        coptic_words = set(verse['coptic'].lower().split())\n",
        "        english_words = set(verse['english'].lower().split())\n",
        "        \n",
        "        # Count co-occurrences\n",
        "        for cop_word in coptic_words:\n",
        "            for eng_word in english_words:\n",
        "                cooccurrence[cop_word][eng_word] += 1\n",
        "    \n",
        "    # Extract high-confidence pairs\n",
        "    word_pairs = []\n",
        "    for cop_word, eng_counts in cooccurrence.items():\n",
        "        # Get most frequent English translation\n",
        "        best_eng = max(eng_counts.items(), key=lambda x: x[1])\n",
        "        if best_eng[1] >= min_count:\n",
        "            word_pairs.append({\n",
        "                'coptic': cop_word,\n",
        "                'english': best_eng[0],\n",
        "                'count': best_eng[1]\n",
        "            })\n",
        "    \n",
        "    # Sort by frequency\n",
        "    word_pairs.sort(key=lambda x: x['count'], reverse=True)\n",
        "    return word_pairs\n",
        "\n",
        "print('Extracting word-level co-occurrences...')\n",
        "word_pairs = extract_word_cooccurrences(parallel_verses, min_count=5)\n",
        "print(f'Extracted {len(word_pairs)} Coptic-English word pairs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show top 50 word pairs\n",
        "print('Top 50 Coptic-English word pairs by frequency:\\n')\n",
        "for i, pair in enumerate(word_pairs[:50]):\n",
        "    print(f\"{i+1:2d}. {pair['coptic']:20s} \u2192 {pair['english']:20s} (count: {pair['count']})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save parallel verses\n",
        "OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:\n",
        "    json.dump(parallel_verses, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f'Saved {len(parallel_verses)} parallel verses to {OUTPUT_PATH}')\n",
        "\n",
        "# Save word pairs\n",
        "WORD_PAIRS_PATH = PROJECT_ROOT / 'data/processed/coptic_english_word_pairs.json'\n",
        "with open(WORD_PAIRS_PATH, 'w', encoding='utf-8') as f:\n",
        "    json.dump(word_pairs, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f'Saved {len(word_pairs)} word pairs to {WORD_PAIRS_PATH}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Successfully extracted Coptic-English parallel data from OPUS Bible corpus.\n",
        "\n",
        "**Next Steps**:\n",
        "1. Download \u010cern\u00fd's Etymological Dictionary\n",
        "2. Extract Egyptian-Coptic cognate pairs\n",
        "3. Map Egyptian \u2192 Coptic \u2192 English to create enhanced anchors"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "heiroglyphy",
      "language": "python",
      "name": "heiroglyphy"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}