{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# V7 FastText + Visual Embeddings: Complete Pipeline\n",
                "\n",
                "## Overview\n",
                "This notebook combines **FastText** text embeddings with **Visual CNN features** to create multimodal hieroglyphic representations, then aligns them with English GloVe embeddings.\n",
                "\n",
                "### Pipeline Steps\n",
                "1. **Data Cleaning**: Extract transliteration from BBAW parquet\n",
                "2. **FastText Training**: Train 768d embeddings on transliteration\n",
                "3. **Visual Fusion**: Concatenate FastText (768d) + Visual (768d) = 1536d\n",
                "4. **Alignment**: Linear regression to map 1536d → 300d English space\n",
                "5. **Evaluation**: Test on anchor pairs\n",
                "\n",
                "### Key Fix\n",
                "✅ Now using `transcription` column (transliteration) instead of `hieroglyphs` (MdC codes) to match anchor vocabulary!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Base Directory: /Users/crashy/Development/heiroglyphy/heiro_v7_FastTextVisual\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import pickle\n",
                "import json\n",
                "import logging\n",
                "from pathlib import Path\n",
                "from tqdm import tqdm\n",
                "from gensim.models import FastText, KeyedVectors\n",
                "from sklearn.linear_model import Ridge\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Configure logging\n",
                "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
                "\n",
                "# Setup paths\n",
                "try:\n",
                "    BASE_DIR = Path(__file__).resolve().parent.parent\n",
                "except NameError:\n",
                "    BASE_DIR = Path.cwd().parent\n",
                "\n",
                "print(f\"Base Directory: {BASE_DIR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Data Cleaning\n",
                "\n",
                "Extract transliteration sequences from the BBAW dataset.\n",
                "\n",
                "**Source**: `heiro_v6_BERT/data/raw/bbaw_huggingface.parquet`  \n",
                "**Column**: `transcription` (transliteration like \"jr,j-pꜥ,t ḥꜣ,tj-ꜥ\")  \n",
                "**Output**: `data/processed/cleaned_corpus.txt`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Reading from /Users/crashy/Development/heiroglyphy/heiro_v6_BERT/data/raw/bbaw_huggingface.parquet...\n",
                        "Total rows: 100736\n",
                        "Columns: ['transcription', 'translation', 'hieroglyphs']\n"
                    ]
                }
            ],
            "source": [
                "# Define paths\n",
                "PARQUET_PATH = BASE_DIR.parent / \"heiro_v6_BERT/data/raw/bbaw_huggingface.parquet\"\n",
                "CLEAN_DATA_PATH = BASE_DIR / \"data/processed/cleaned_corpus.txt\"\n",
                "\n",
                "# Ensure output directory exists\n",
                "CLEAN_DATA_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "print(f\"Reading from {PARQUET_PATH}...\")\n",
                "df = pd.read_parquet(PARQUET_PATH)\n",
                "\n",
                "print(f\"Total rows: {len(df)}\")\n",
                "print(f\"Columns: {df.columns.tolist()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Rows with transcription: 100729\n",
                        "\n",
                        "Sample transcriptions:\n",
                        "['⸢pḏ,wt-9⸣   n =f   [⸮ḥtr?]   ⸢m⸣  ', 'ḥtr tp,j ꜥꜣ n ḥm =f Ꜥꜣ-nḫt,w', '⸢wr⸣.pl ⸢ꜥꜣi̯⸣.pl n.w ⸢Rṯn,w⸣ ⸢jni̯⸣ ⸢ḥm⸣ ⸢=f⸣ ⸢m⸣ ⸢sqr-ꜥnḫ⸣']\n"
                    ]
                }
            ],
            "source": [
                "# Filter for rows with non-empty transcription\n",
                "df_trans = df[df['transcription'].notna() & (df['transcription'] != '')]\n",
                "print(f\"Rows with transcription: {len(df_trans)}\")\n",
                "\n",
                "# Show a sample\n",
                "print(\"\\nSample transcriptions:\")\n",
                "print(df_trans['transcription'].head(3).tolist())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Processing transcriptions...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 100729/100729 [00:00<00:00, 1037042.61it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Extracted 100729 lines.\n",
                        "Saved cleaned corpus to /Users/crashy/Development/heiroglyphy/heiro_v7_FastTextVisual/data/processed/cleaned_corpus.txt\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Clean and tokenize\n",
                "cleaned_lines = []\n",
                "\n",
                "print(\"Processing transcriptions...\")\n",
                "for trans_str in tqdm(df_trans['transcription']):\n",
                "    if not isinstance(trans_str, str):\n",
                "        continue\n",
                "    \n",
                "    # Normalize whitespace (transliteration is already space-separated)\n",
                "    clean_line = \" \".join(trans_str.split())\n",
                "    \n",
                "    if clean_line:\n",
                "        cleaned_lines.append(clean_line)\n",
                "\n",
                "print(f\"Extracted {len(cleaned_lines)} lines.\")\n",
                "\n",
                "# Save to file\n",
                "with open(CLEAN_DATA_PATH, 'w', encoding='utf-8') as f:\n",
                "    for line in cleaned_lines:\n",
                "        f.write(line + \"\\n\")\n",
                "\n",
                "print(f\"Saved cleaned corpus to {CLEAN_DATA_PATH}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "First 3 lines of cleaned corpus:\n",
                        "1: ⸢pḏ,wt-9⸣ n =f [⸮ḥtr?] ⸢m⸣...\n",
                        "2: ḥtr tp,j ꜥꜣ n ḥm =f Ꜥꜣ-nḫt,w...\n",
                        "3: ⸢wr⸣.pl ⸢ꜥꜣi̯⸣.pl n.w ⸢Rṯn,w⸣ ⸢jni̯⸣ ⸢ḥm⸣ ⸢=f⸣ ⸢m⸣ ⸢sqr-ꜥnḫ⸣...\n"
                    ]
                }
            ],
            "source": [
                "# Verify output\n",
                "print(\"\\nFirst 3 lines of cleaned corpus:\")\n",
                "with open(CLEAN_DATA_PATH, 'r', encoding='utf-8') as f:\n",
                "    for i, line in enumerate(f):\n",
                "        if i >= 3:\n",
                "            break\n",
                "        print(f\"{i+1}: {line.strip()[:100]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: FastText Training\n",
                "\n",
                "Train 300d FastText embeddings on the transliteration corpus."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training FastText model on /Users/crashy/Development/heiroglyphy/heiro_v7_FastTextVisual/data/processed/cleaned_corpus.txt...\n"
                    ]
                }
            ],
            "source": [
                "# Define paths\n",
                "MODEL_DIR = BASE_DIR / \"models\"\n",
                "MODEL_PATH = MODEL_DIR / \"fasttext_v7.model\"\n",
                "\n",
                "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "print(f\"Training FastText model on {CLEAN_DATA_PATH}...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-11-21 14:20:23,575 : INFO : collecting all words and their counts\n",
                        "2025-11-21 14:20:23,577 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
                        "2025-11-21 14:20:23,599 : INFO : PROGRESS: at sentence #10000, processed 95884 words, keeping 16423 word types\n",
                        "2025-11-21 14:20:23,621 : INFO : PROGRESS: at sentence #20000, processed 177018 words, keeping 27382 word types\n",
                        "2025-11-21 14:20:23,640 : INFO : PROGRESS: at sentence #30000, processed 258107 words, keeping 36395 word types\n",
                        "2025-11-21 14:20:23,656 : INFO : PROGRESS: at sentence #40000, processed 329546 words, keeping 40659 word types\n",
                        "2025-11-21 14:20:23,673 : INFO : PROGRESS: at sentence #50000, processed 397825 words, keeping 49010 word types\n",
                        "2025-11-21 14:20:23,688 : INFO : PROGRESS: at sentence #60000, processed 453602 words, keeping 55475 word types\n",
                        "2025-11-21 14:20:23,716 : INFO : PROGRESS: at sentence #70000, processed 519943 words, keeping 61231 word types\n",
                        "2025-11-21 14:20:23,736 : INFO : PROGRESS: at sentence #80000, processed 593196 words, keeping 67224 word types\n",
                        "2025-11-21 14:20:23,755 : INFO : PROGRESS: at sentence #90000, processed 676015 words, keeping 72885 word types\n",
                        "2025-11-21 14:20:23,778 : INFO : PROGRESS: at sentence #100000, processed 781525 words, keeping 80168 word types\n",
                        "2025-11-21 14:20:23,781 : INFO : collected 80662 word types from a corpus of 789159 raw words and 100729 sentences\n",
                        "2025-11-21 14:20:23,781 : INFO : Creating a fresh vocabulary\n",
                        "2025-11-21 14:20:23,917 : INFO : FastText lifecycle event {'msg': 'effective_min_count=1 retains 80662 unique words (100.00% of original 80662, drops 0)', 'datetime': '2025-11-21T14:20:23.917085', 'gensim': '4.4.0', 'python': '3.12.3 (main, Jun  1 2025, 04:19:33) [Clang 17.0.0 (clang-1700.0.13.5)]', 'platform': 'macOS-26.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
                        "2025-11-21 14:20:23,917 : INFO : FastText lifecycle event {'msg': 'effective_min_count=1 leaves 789159 word corpus (100.00% of original 789159, drops 0)', 'datetime': '2025-11-21T14:20:23.917633', 'gensim': '4.4.0', 'python': '3.12.3 (main, Jun  1 2025, 04:19:33) [Clang 17.0.0 (clang-1700.0.13.5)]', 'platform': 'macOS-26.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
                        "2025-11-21 14:20:24,112 : INFO : deleting the raw counts dictionary of 80662 items\n",
                        "2025-11-21 14:20:24,113 : INFO : sample=0.001 downsamples 36 most-common words\n",
                        "2025-11-21 14:20:24,113 : INFO : FastText lifecycle event {'msg': 'downsampling leaves estimated 631349.4449474833 word corpus (80.0%% of prior 789159)', 'datetime': '2025-11-21T14:20:24.113723', 'gensim': '4.4.0', 'python': '3.12.3 (main, Jun  1 2025, 04:19:33) [Clang 17.0.0 (clang-1700.0.13.5)]', 'platform': 'macOS-26.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
                        "2025-11-21 14:20:24,912 : INFO : estimated required memory for 80662 words, 2000000 buckets and 768 dimensions: 6697537168 bytes\n",
                        "2025-11-21 14:20:24,913 : INFO : resetting layer weights\n",
                        "2025-11-21 14:20:34,521 : INFO : FastText lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-11-21T14:20:34.521284', 'gensim': '4.4.0', 'python': '3.12.3 (main, Jun  1 2025, 04:19:33) [Clang 17.0.0 (clang-1700.0.13.5)]', 'platform': 'macOS-26.1-arm64-arm-64bit', 'event': 'build_vocab'}\n",
                        "2025-11-21 14:20:34,522 : INFO : FastText lifecycle event {'msg': 'training model with 3 workers on 80662 vocabulary and 768 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-11-21T14:20:34.521989', 'gensim': '4.4.0', 'python': '3.12.3 (main, Jun  1 2025, 04:19:33) [Clang 17.0.0 (clang-1700.0.13.5)]', 'platform': 'macOS-26.1-arm64-arm-64bit', 'event': 'train'}\n",
                        "2025-11-21 14:20:35,541 : INFO : EPOCH 0 - PROGRESS: at 10.41% examples, 76853 words/s, in_qsize 6, out_qsize 0\n",
                        "2025-11-21 14:20:36,579 : INFO : EPOCH 0 - PROGRESS: at 25.96% examples, 87897 words/s, in_qsize 6, out_qsize 0\n",
                        "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
                        "2025-11-21 14:20:37,698 : INFO : EPOCH 0 - PROGRESS: at 44.59% examples, 90594 words/s, in_qsize 6, out_qsize 0\n",
                        "2025-11-21 14:20:38,731 : INFO : EPOCH 0 - PROGRESS: at 62.28% examples, 89418 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:20:39,760 : INFO : EPOCH 0 - PROGRESS: at 80.42% examples, 91705 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:20:40,823 : INFO : EPOCH 0 - PROGRESS: at 93.95% examples, 91342 words/s, in_qsize 6, out_qsize 0\n",
                        "2025-11-21 14:20:41,504 : INFO : EPOCH 0: training on 789159 raw words (631252 effective words) took 7.0s, 90415 effective words/s\n",
                        "2025-11-21 14:20:42,526 : INFO : EPOCH 1 - PROGRESS: at 11.76% examples, 84421 words/s, in_qsize 5, out_qsize 0\n",
                        "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
                        "2025-11-21 14:20:43,656 : INFO : EPOCH 1 - PROGRESS: at 27.34% examples, 87640 words/s, in_qsize 6, out_qsize 0\n",
                        "2025-11-21 14:20:44,757 : INFO : EPOCH 1 - PROGRESS: at 46.48% examples, 91252 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:20:45,778 : INFO : EPOCH 1 - PROGRESS: at 62.38% examples, 88122 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:20:46,828 : INFO : EPOCH 1 - PROGRESS: at 81.55% examples, 91801 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:20:47,887 : INFO : EPOCH 1 - PROGRESS: at 94.69% examples, 91428 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:20:48,482 : INFO : EPOCH 1: training on 789159 raw words (631329 effective words) took 7.0s, 90488 effective words/s\n",
                        "2025-11-21 14:20:49,630 : INFO : EPOCH 2 - PROGRESS: at 12.97% examples, 82072 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:20:50,640 : INFO : EPOCH 2 - PROGRESS: at 28.66% examples, 91142 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:20:51,748 : INFO : EPOCH 2 - PROGRESS: at 48.22% examples, 93580 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:20:52,981 : INFO : EPOCH 2 - PROGRESS: at 58.83% examples, 79731 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:20:54,111 : INFO : EPOCH 2 - PROGRESS: at 75.56% examples, 80993 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:20:55,193 : INFO : EPOCH 2 - PROGRESS: at 92.08% examples, 83436 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:20:56,183 : INFO : EPOCH 2: training on 789159 raw words (631153 effective words) took 7.7s, 81970 effective words/s\n",
                        "2025-11-21 14:20:57,343 : INFO : EPOCH 3 - PROGRESS: at 10.41% examples, 67530 words/s, in_qsize 6, out_qsize 0\n",
                        "2025-11-21 14:20:58,357 : INFO : EPOCH 3 - PROGRESS: at 24.72% examples, 79647 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:20:59,457 : INFO : EPOCH 3 - PROGRESS: at 44.59% examples, 87972 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:00,596 : INFO : EPOCH 3 - PROGRESS: at 60.64% examples, 83336 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:01,654 : INFO : EPOCH 3 - PROGRESS: at 78.47% examples, 86316 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:02,747 : INFO : EPOCH 3 - PROGRESS: at 93.95% examples, 87747 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:03,461 : INFO : EPOCH 3: training on 789159 raw words (631465 effective words) took 7.3s, 86785 effective words/s\n",
                        "2025-11-21 14:21:04,470 : INFO : EPOCH 4 - PROGRESS: at 11.76% examples, 85473 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:05,583 : INFO : EPOCH 4 - PROGRESS: at 27.34% examples, 88841 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:06,699 : INFO : EPOCH 4 - PROGRESS: at 46.33% examples, 91698 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:07,734 : INFO : EPOCH 4 - PROGRESS: at 64.02% examples, 90250 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:08,803 : INFO : EPOCH 4 - PROGRESS: at 82.71% examples, 92998 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:09,960 : INFO : EPOCH 4 - PROGRESS: at 95.51% examples, 91022 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:10,317 : INFO : EPOCH 4: training on 789159 raw words (631689 effective words) took 6.9s, 92150 effective words/s\n",
                        "2025-11-21 14:21:11,332 : INFO : EPOCH 5 - PROGRESS: at 11.76% examples, 84934 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:12,371 : INFO : EPOCH 5 - PROGRESS: at 25.96% examples, 88000 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:13,388 : INFO : EPOCH 5 - PROGRESS: at 44.59% examples, 93699 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:14,411 : INFO : EPOCH 5 - PROGRESS: at 60.64% examples, 89750 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:15,594 : INFO : EPOCH 5 - PROGRESS: at 80.42% examples, 91047 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:16,800 : INFO : EPOCH 5 - PROGRESS: at 95.51% examples, 91164 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:17,181 : INFO : EPOCH 5: training on 789159 raw words (631223 effective words) took 6.9s, 91969 effective words/s\n",
                        "2025-11-21 14:21:18,185 : INFO : EPOCH 6 - PROGRESS: at 11.76% examples, 85868 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:19,195 : INFO : EPOCH 6 - PROGRESS: at 24.72% examples, 85840 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:20,257 : INFO : EPOCH 6 - PROGRESS: at 44.59% examples, 93642 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:21,299 : INFO : EPOCH 6 - PROGRESS: at 60.64% examples, 89308 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:22,348 : INFO : EPOCH 6 - PROGRESS: at 76.41% examples, 89721 words/s, in_qsize 6, out_qsize 0\n",
                        "2025-11-21 14:21:23,475 : INFO : EPOCH 6 - PROGRESS: at 92.82% examples, 90232 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:24,170 : INFO : EPOCH 6: training on 789159 raw words (631553 effective words) took 7.0s, 90382 effective words/s\n",
                        "2025-11-21 14:21:25,220 : INFO : EPOCH 7 - PROGRESS: at 11.76% examples, 82236 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:26,236 : INFO : EPOCH 7 - PROGRESS: at 24.89% examples, 83846 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:27,247 : INFO : EPOCH 7 - PROGRESS: at 44.59% examples, 93555 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:28,437 : INFO : EPOCH 7 - PROGRESS: at 62.19% examples, 88067 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:29,511 : INFO : EPOCH 7 - PROGRESS: at 81.55% examples, 91491 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:30,591 : INFO : EPOCH 7 - PROGRESS: at 95.51% examples, 92110 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:31,146 : INFO : EPOCH 7: training on 789159 raw words (631597 effective words) took 7.0s, 90555 effective words/s\n",
                        "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
                        "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
                        "2025-11-21 14:21:32,231 : INFO : EPOCH 8 - PROGRESS: at 11.76% examples, 79479 words/s, in_qsize 6, out_qsize 0\n",
                        "2025-11-21 14:21:33,303 : INFO : EPOCH 8 - PROGRESS: at 24.72% examples, 80166 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:34,309 : INFO : EPOCH 8 - PROGRESS: at 42.50% examples, 85965 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:35,535 : INFO : EPOCH 8 - PROGRESS: at 56.99% examples, 79657 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:36,665 : INFO : EPOCH 8 - PROGRESS: at 75.56% examples, 82568 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:37,673 : INFO : EPOCH 8 - PROGRESS: at 92.08% examples, 85783 words/s, in_qsize 5, out_qsize 0\n",
                        "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
                        "2025-11-21 14:21:38,554 : INFO : EPOCH 8: training on 789159 raw words (631285 effective words) took 7.4s, 85227 effective words/s\n",
                        "2025-11-21 14:21:39,586 : INFO : EPOCH 9 - PROGRESS: at 10.41% examples, 75875 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:40,602 : INFO : EPOCH 9 - PROGRESS: at 24.72% examples, 84515 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:41,676 : INFO : EPOCH 9 - PROGRESS: at 44.59% examples, 92204 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:42,701 : INFO : EPOCH 9 - PROGRESS: at 58.83% examples, 86509 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:43,743 : INFO : EPOCH 9 - PROGRESS: at 75.56% examples, 87894 words/s, in_qsize 6, out_qsize 0\n",
                        "2025-11-21 14:21:44,775 : INFO : EPOCH 9 - PROGRESS: at 92.08% examples, 90051 words/s, in_qsize 5, out_qsize 0\n",
                        "2025-11-21 14:21:45,674 : INFO : EPOCH 9: training on 789159 raw words (631475 effective words) took 7.1s, 88695 effective words/s\n",
                        "2025-11-21 14:21:45,675 : INFO : FastText lifecycle event {'msg': 'training on 7891590 raw words (6314021 effective words) took 71.2s, 88738 effective words/s', 'datetime': '2025-11-21T14:21:45.675359', 'gensim': '4.4.0', 'python': '3.12.3 (main, Jun  1 2025, 04:19:33) [Clang 17.0.0 (clang-1700.0.13.5)]', 'platform': 'macOS-26.1-arm64-arm-64bit', 'event': 'train'}\n",
                        "2025-11-21 14:21:48,254 : INFO : FastText lifecycle event {'params': 'FastText<vocab=80662, vector_size=768, alpha=0.025>', 'datetime': '2025-11-21T14:21:48.254971', 'gensim': '4.4.0', 'python': '3.12.3 (main, Jun  1 2025, 04:19:33) [Clang 17.0.0 (clang-1700.0.13.5)]', 'platform': 'macOS-26.1-arm64-arm-64bit', 'event': 'created'}\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Vocabulary size: 80662\n"
                    ]
                }
            ],
            "source": [
                "# Load corpus\n",
                "class MyCorpus:\n",
                "    def __iter__(self):\n",
                "        with open(CLEAN_DATA_PATH, 'r', encoding='utf-8') as f:\n",
                "            for line in f:\n",
                "                yield line.split()\n",
                "\n",
                "sentences = MyCorpus()\n",
                "\n",
                "# Train FastText\n",
                "# Parameters:\n",
                "# vector_size=768: Standard size, matches GloVe\n",
                "# window=5: Context window\n",
                "# min_count=1: Keep all words for now\n",
                "# sg=1: Skip-gram (usually better for smaller datasets)\n",
                "# epochs=10: Train for a bit longer\n",
                "model = FastText(vector_size=768, window=5, min_count=1, sentences=sentences, epochs=10, sg=1)\n",
                "\n",
                "print(f\"\\nVocabulary size: {len(model.wv)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-11-21 14:18:25,079 : INFO : FastText lifecycle event {'fname_or_handle': '/Users/crashy/Development/heiroglyphy/heiro_v7_FastTextVisual/models/fasttext_v7.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-11-21T14:18:25.079394', 'gensim': '4.4.0', 'python': '3.12.3 (main, Jun  1 2025, 04:19:33) [Clang 17.0.0 (clang-1700.0.13.5)]', 'platform': 'macOS-26.1-arm64-arm-64bit', 'event': 'saving'}\n",
                        "2025-11-21 14:18:25,079 : INFO : not storing attribute vectors\n",
                        "2025-11-21 14:18:25,080 : INFO : storing np array 'vectors_vocab' to /Users/crashy/Development/heiroglyphy/heiro_v7_FastTextVisual/models/fasttext_v7.model.wv.vectors_vocab.npy\n",
                        "2025-11-21 14:18:25,098 : INFO : storing np array 'vectors_ngrams' to /Users/crashy/Development/heiroglyphy/heiro_v7_FastTextVisual/models/fasttext_v7.model.wv.vectors_ngrams.npy\n",
                        "2025-11-21 14:18:25,569 : INFO : not storing attribute buckets_word\n",
                        "2025-11-21 14:18:25,570 : INFO : storing np array 'syn1neg' to /Users/crashy/Development/heiroglyphy/heiro_v7_FastTextVisual/models/fasttext_v7.model.syn1neg.npy\n",
                        "2025-11-21 14:18:25,588 : INFO : not storing attribute cum_table\n",
                        "2025-11-21 14:18:25,607 : INFO : saved /Users/crashy/Development/heiroglyphy/heiro_v7_FastTextVisual/models/fasttext_v7.model\n",
                        "2025-11-21 14:18:25,638 : INFO : storing 80662x300 projection weights into /Users/crashy/Development/heiroglyphy/heiro_v7_FastTextVisual/models/fasttext_v7.vec\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model saved to /Users/crashy/Development/heiroglyphy/heiro_v7_FastTextVisual/models/fasttext_v7.model\n",
                        "Vectors saved to /Users/crashy/Development/heiroglyphy/heiro_v7_FastTextVisual/models/fasttext_v7.vec\n"
                    ]
                }
            ],
            "source": [
                "# Save model\n",
                "model.save(str(MODEL_PATH))\n",
                "print(f\"Model saved to {MODEL_PATH}\")\n",
                "\n",
                "# Save vectors in word2vec format for easy inspection\n",
                "model.wv.save_word2vec_format(str(MODEL_DIR / \"fasttext_v7.vec\"))\n",
                "print(f\"Vectors saved to {MODEL_DIR / 'fasttext_v7.vec'}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test similarities with common transliteration tokens\n",
                "test_words = [\"n\", \"m\", \"r\", \"ḥr,w\", \"nṯr\"]\n",
                "for word in test_words:\n",
                "    if word in model.wv:\n",
                "        print(f\"\\nMost similar to '{word}':\")\n",
                "        print(model.wv.most_similar(word, topn=5))\n",
                "    else:\n",
                "        print(f\"\\n'{word}' not in vocabulary.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Visual Embedding Fusion\n",
                "\n",
                "Combine FastText vectors with pre-computed visual embeddings from V6.\n",
                "\n",
                "**Note**: Visual embeddings are keyed by Unicode, so we'll use the lexicon to map transliteration → Unicode → visual features."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load FastText Model\n",
                "print(f\"Loading FastText model from {MODEL_PATH}...\")\n",
                "ft_model = FastText.load(str(MODEL_PATH))\n",
                "ft_wv = ft_model.wv\n",
                "print(f\"FastText Vocab Size: {len(ft_wv)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Visual Embeddings\n",
                "VISUAL_EMBED_PATH = BASE_DIR.parent / \"heiro_v6_BERT/data/processed/visual_embeddings_768d.pkl\"\n",
                "print(f\"Loading Visual embeddings from {VISUAL_EMBED_PATH}...\")\n",
                "\n",
                "with open(VISUAL_EMBED_PATH, 'rb') as f:\n",
                "    visual_embeds = pickle.load(f)\n",
                "    \n",
                "print(f\"Visual Embeddings Size: {len(visual_embeds)}\")\n",
                "print(f\"Sample key: {list(visual_embeds.keys())[0]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Lexicon for Mapping\n",
                "LEXICON_PATH = BASE_DIR.parent / \"heiro_v6_BERT/data/processed/hieroglyph_lexicon.csv\"\n",
                "print(f\"Loading Lexicon from {LEXICON_PATH}...\")\n",
                "\n",
                "lexicon_df = pd.read_csv(LEXICON_PATH)\n",
                "print(f\"Lexicon size: {len(lexicon_df)}\")\n",
                "print(f\"\\nSample entries:\")\n",
                "print(lexicon_df.head())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create mapping: glyph_name (lowercase) -> unicode\n",
                "gardiner_to_unicode = dict(zip(lexicon_df['glyph_name'], lexicon_df['unicode']))\n",
                "print(f\"Created mapping for {len(gardiner_to_unicode)} glyphs\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fuse Embeddings\n",
                "print(\"Fusing embeddings...\")\n",
                "fused_vectors = []\n",
                "words = []\n",
                "\n",
                "visual_dim = 768\n",
                "text_dim = 768\n",
                "\n",
                "matches = 0\n",
                "misses = 0\n",
                "\n",
                "for word in ft_wv.index_to_key:\n",
                "    # Get text vector\n",
                "    text_vec = ft_wv[word]\n",
                "    \n",
                "    # Get visual vector (default to zeros if not found)\n",
                "    visual_vec = np.zeros(visual_dim, dtype=np.float32)\n",
                "    \n",
                "    # Try to match word to visual embedding\n",
                "    # For transliteration, this is harder - we'd need a transliteration->glyph mapping\n",
                "    # For now, we'll just use zero vectors (no visual info)\n",
                "    # This is a limitation we should note!\n",
                "    \n",
                "    # Normalize vectors (L2)\n",
                "    text_norm = np.linalg.norm(text_vec)\n",
                "    if text_norm > 0:\n",
                "        text_vec = text_vec / text_norm\n",
                "        \n",
                "    visual_norm = np.linalg.norm(visual_vec)\n",
                "    if visual_norm > 0:\n",
                "        visual_vec = visual_vec / visual_norm\n",
                "        matches += 1\n",
                "    else:\n",
                "        misses += 1\n",
                "        \n",
                "    # Concatenate\n",
                "    fused_vec = np.concatenate([text_vec, visual_vec])\n",
                "    fused_vectors.append(fused_vec)\n",
                "    words.append(word)\n",
                "    \n",
                "print(f\"Fusion Complete. Matches: {matches}, Misses: {misses}\")\n",
                "print(f\"Match Rate: {matches / len(words):.2%}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save Fused Model\n",
                "FUSED_MODEL_PATH = BASE_DIR / \"models/fused_embeddings_1536d.kv\"\n",
                "\n",
                "fused_vectors = np.array(fused_vectors)\n",
                "print(f\"Fused Vectors Shape: {fused_vectors.shape}\")\n",
                "\n",
                "kv = KeyedVectors(vector_size=text_dim + visual_dim)\n",
                "kv.add_vectors(words, fused_vectors)\n",
                "\n",
                "kv.save(str(FUSED_MODEL_PATH))\n",
                "print(f\"Saved fused model to {FUSED_MODEL_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Alignment & Evaluation\n",
                "\n",
                "Align the fused embeddings to English GloVe space using Linear Regression."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Fused Model\n",
                "print(f\"Loading Fused Model from {FUSED_MODEL_PATH}...\")\n",
                "hiero_kv = KeyedVectors.load(str(FUSED_MODEL_PATH))\n",
                "print(f\"Loaded {len(hiero_kv)} hieroglyphic vectors\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load GloVe\n",
                "GLOVE_PATH = BASE_DIR.parent / \"heiro_v5_getdata/data/processed/glove.6B.300d.txt\"\n",
                "print(f\"Loading GloVe from {GLOVE_PATH}...\")\n",
                "print(\"(This may take a minute...)\")\n",
                "\n",
                "glove_kv = KeyedVectors.load_word2vec_format(str(GLOVE_PATH), binary=False, no_header=True)\n",
                "print(f\"Loaded {len(glove_kv)} English vectors\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Anchors\n",
                "ANCHORS_PATH = BASE_DIR.parent / \"heiro_v6_BERT/data/processed/anchors.json\"\n",
                "print(f\"Loading Anchors from {ANCHORS_PATH}...\")\n",
                "\n",
                "with open(ANCHORS_PATH, 'r') as f:\n",
                "    anchors = json.load(f)\n",
                "    \n",
                "print(f\"Loaded {len(anchors)} anchor pairs\")\n",
                "print(f\"\\nSample anchors:\")\n",
                "for i in range(min(3, len(anchors))):\n",
                "    print(f\"  {anchors[i]['hieroglyphic']} → {anchors[i]['english']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare Alignment Data\n",
                "print(\"Preparing alignment data...\")\n",
                "X = []\n",
                "Y = []\n",
                "valid_anchors = []\n",
                "\n",
                "for anchor in anchors:\n",
                "    h_word = anchor['hieroglyphic']\n",
                "    e_word = anchor['english'].lower()  # GloVe is lowercase\n",
                "    \n",
                "    # Check if words exist\n",
                "    if h_word in hiero_kv and e_word in glove_kv:\n",
                "        X.append(hiero_kv[h_word])\n",
                "        Y.append(glove_kv[e_word])\n",
                "        valid_anchors.append((h_word, e_word))\n",
                "        \n",
                "X = np.array(X)\n",
                "Y = np.array(Y)\n",
                "\n",
                "print(f\"Valid Anchors: {len(X)} / {len(anchors)} ({len(X)/len(anchors)*100:.1f}%)\")\n",
                "print(f\"\\nThis is the KEY metric! We need good vocabulary overlap.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split Data\n",
                "if len(X) > 10:  # Only split if we have enough data\n",
                "    X_train, X_test, Y_train, Y_test, anchors_train, anchors_test = train_test_split(\n",
                "        X, Y, valid_anchors, test_size=0.2, random_state=42\n",
                "    )\n",
                "    print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")\n",
                "else:\n",
                "    print(f\"⚠️  WARNING: Only {len(X)} valid anchors! Need more for reliable evaluation.\")\n",
                "    X_train, Y_train = X, Y\n",
                "    X_test, Y_test = X, Y\n",
                "    anchors_test = valid_anchors"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Alignment (Linear Regression / Ridge)\n",
                "print(\"Training Linear Alignment...\")\n",
                "aligner = Ridge(alpha=1.0)\n",
                "aligner.fit(X_train, Y_train)\n",
                "\n",
                "print(f\"R² Score on Train: {aligner.score(X_train, Y_train):.4f}\")\n",
                "print(f\"R² Score on Test: {aligner.score(X_test, Y_test):.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate\n",
                "print(\"Evaluating on Test Set...\")\n",
                "correct_top1 = 0\n",
                "correct_top5 = 0\n",
                "correct_top10 = 0\n",
                "total = len(X_test)\n",
                "\n",
                "# Predict all test vectors\n",
                "Y_pred = aligner.predict(X_test)\n",
                "\n",
                "for i in tqdm(range(total)):\n",
                "    pred_vec = Y_pred[i]\n",
                "    true_word = anchors_test[i][1]\n",
                "    \n",
                "    # Find nearest neighbors in GloVe\n",
                "    neighbors = glove_kv.similar_by_vector(pred_vec, topn=10)\n",
                "    neighbor_words = [w for w, s in neighbors]\n",
                "    \n",
                "    if true_word == neighbor_words[0]:\n",
                "        correct_top1 += 1\n",
                "    if true_word in neighbor_words[:5]:\n",
                "        correct_top5 += 1\n",
                "    if true_word in neighbor_words[:10]:\n",
                "        correct_top10 += 1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate and display results\n",
                "acc_top1 = correct_top1 / total * 100\n",
                "acc_top5 = correct_top5 / total * 100\n",
                "acc_top10 = correct_top10 / total * 100\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"V7 FastText + Visual Embeddings Results\")\n",
                "print(\"=\"*70)\n",
                "print(f\"Test Samples: {total}\")\n",
                "print(f\"Valid Anchors: {len(X)} / {len(anchors)} ({len(X)/len(anchors)*100:.1f}%)\")\n",
                "print()\n",
                "print(f\"Top-1 Accuracy:  {acc_top1:.2f}%\")\n",
                "print(f\"Top-5 Accuracy:  {acc_top5:.2f}%\")\n",
                "print(f\"Top-10 Accuracy: {acc_top10:.2f}%\")\n",
                "print()\n",
                "print(f\"R² Score (Train): {aligner.score(X_train, Y_train):.4f}\")\n",
                "print(f\"R² Score (Test):  {aligner.score(X_test, Y_test):.4f}\")\n",
                "print()\n",
                "print(\"Comparison:\")\n",
                "print(f\"  V5 Baseline: 24.53%\")\n",
                "print(f\"  V6 BERT:     0.47%\")\n",
                "print(f\"  V7 (This):   {acc_top1:.2f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save results\n",
                "RESULTS_PATH = BASE_DIR / \"data/processed/alignment_results_v7.json\"\n",
                "\n",
                "results = {\n",
                "    \"model\": \"V7 FastText + Visuals (Fused 1536d -> 300d)\",\n",
                "    \"test_samples\": total,\n",
                "    \"valid_anchors\": len(X),\n",
                "    \"total_anchors\": len(anchors),\n",
                "    \"anchor_coverage\": len(X) / len(anchors) * 100,\n",
                "    \"top1_accuracy\": acc_top1,\n",
                "    \"top5_accuracy\": acc_top5,\n",
                "    \"top10_accuracy\": acc_top10,\n",
                "    \"r2_train\": aligner.score(X_train, Y_train),\n",
                "    \"r2_test\": aligner.score(X_test, Y_test)\n",
                "}\n",
                "\n",
                "with open(RESULTS_PATH, 'w') as f:\n",
                "    json.dump(results, f, indent=2)\n",
                "    \n",
                "print(f\"\\nSaved results to {RESULTS_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Analysis\n",
                "\n",
                "### Key Metrics to Watch\n",
                "1. **Anchor Coverage**: What % of anchors have valid vocabulary matches?\n",
                "2. **Top-1 Accuracy**: Does the model predict the correct English word?\n",
                "3. **R² Score**: How well does the linear transformation fit?\n",
                "\n",
                "### Expected Improvement\n",
                "By using `transcription` instead of `hieroglyphs`, we should see:\n",
                "- ✅ **Much higher anchor coverage** (from 0.18% to ~87%)\n",
                "- ✅ **Meaningful accuracy** (hopefully approaching V5's 24.53%)\n",
                "- ⚠️ **No visual information** (transliteration doesn't map to glyphs easily)\n",
                "\n",
                "### Limitations\n",
                "- Visual embeddings are keyed by Unicode/Gardiner codes, not transliteration\n",
                "- We're effectively training text-only FastText (visual vectors are zeros)\n",
                "- To truly leverage visual features, we'd need a transliteration → glyph mapping"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python (spontaneous-remission)",
            "language": "python",
            "name": "spontaneous-remission"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
