{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# V7 Phase 1: Data Cleaning & Tokenization\n",
                "\n",
                "## Goal\n",
                "Prepare a clean, high-quality corpus for FastText training. \n",
                "We need to remove non-glyph artifacts (like line numbers, headers) that might have polluted previous models.\n",
                "\n",
                "## Strategy\n",
                "1. **Load Data**: `data/raw/all_data.json` (104k texts).\n",
                "2. **Clean**: Apply regex to remove:\n",
                "    - Line numbers (e.g., \"1.\", \"[1]\")\n",
                "    - English text/comments\n",
                "    - Punctuation not part of MdC (Manuel de Codage).\n",
                "3. **Tokenize**: Ensure space-separated glyphs for FastText.\n",
                "4. **Export**: Save as `data/processed/cleaned_corpus.txt`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import re\n",
                "from pathlib import Path\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Paths\n",
                "RAW_DATA_PATH = Path(\"../data/raw/all_data.json\")\n",
                "CLEAN_DATA_PATH = Path(\"../data/processed/cleaned_corpus.txt\")\n",
                "\n",
                "# Ensure output dir exists\n",
                "CLEAN_DATA_PATH.parent.mkdir(parents=True, exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "ename": "FileNotFoundError",
                    "evalue": "[Errno 2] No such file or directory: 'data/raw/all_data.json'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mRAW_DATA_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      2\u001b[39m     raw_data = json.load(f)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(raw_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m texts.\u001b[39m\u001b[33m\"\u001b[39m)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:327\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    322\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    325\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/raw/all_data.json'"
                    ]
                }
            ],
            "source": [
                "with open(RAW_DATA_PATH, 'r') as f:\n",
                "    raw_data = json.load(f)\n",
                "\n",
                "print(f\"Loaded {len(raw_data)} texts.\")\n",
                "print(\"Sample raw text:\", raw_data[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Cleaning Logic"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def clean_hieroglyphs(text):\n",
                "    if not isinstance(text, str):\n",
                "        return \"\"\n",
                "    \n",
                "    # 1. Remove Line Numbers & Brackets (e.g., \"[1]\", \"1.\", \"<...>\")\n",
                "    text = re.sub(r'\\[.*?\\]', '', text)  # Remove [1], [2a]\n",
                "    text = re.sub(r'<.*?>', '', text)     # Remove <...>\n",
                "    text = re.sub(r'\\(.*?\\)', '', text)     # Remove (...)\n",
                "    \n",
                "    # 2. Remove digits that are likely line numbers (start of line or standalone)\n",
                "    text = re.sub(r'^\\d+\\.', '', text)    # \"1. ...\"\n",
                "    text = re.sub(r'\\s\\d+\\.', ' ', text)  # \" ... 2. ...\"\n",
                "    \n",
                "    # 3. Remove non-MdC characters (keep A-Z, a-z, 0-9, -, *, :, etc.)\n",
                "    # This is aggressive; we might want to be more careful if MdC uses special chars.\n",
                "    # For now, let's focus on removing obvious English words.\n",
                "    # A heuristic: if a token is > 1 char and contains only lowercase letters, it's likely English garbage.\n",
                "    # (MdC usually uses uppercase for Gardiner codes like A1, N35)\n",
                "    \n",
                "    tokens = text.split()\n",
                "    clean_tokens = []\n",
                "    for t in tokens:\n",
                "        # Filter out likely English words (all lowercase alpha, length > 1)\n",
                "        if t.isalpha() and t.islower() and len(t) > 1:\n",
                "            continue\n",
                "        # Filter out standalone numbers\n",
                "        if t.isdigit():\n",
                "            continue\n",
                "        clean_tokens.append(t)\n",
                "        \n",
                "    return \" \".join(clean_tokens)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Process & Export"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cleaned_lines = []\n",
                "skipped = 0\n",
                "\n",
                "for item in tqdm(raw_data):\n",
                "    # Depending on structure, text might be in 'content', 'hiero', or just the item itself\n",
                "    # Adjust based on actual data structure (assuming list of strings or dicts)\n",
                "    text = item.get('hieroglyphs', '') if isinstance(item, dict) else item\n",
                "    \n",
                "    clean_text = clean_hieroglyphs(text)\n",
                "    if len(clean_text.strip()) > 0:\n",
                "        cleaned_lines.append(clean_text)\n",
                "    else:\n",
                "        skipped += 1\n",
                "\n",
                "print(f\"Processed {len(cleaned_lines)} lines. Skipped {skipped} empty lines.\")\n",
                "\n",
                "# Save to file\n",
                "with open(CLEAN_DATA_PATH, 'w') as f:\n",
                "    for line in cleaned_lines:\n",
                "        f.write(line + \"\\n\")\n",
                "        \n",
                "print(f\"Saved cleaned corpus to {CLEAN_DATA_PATH}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python (spontaneous-remission)",
            "language": "python",
            "name": "spontaneous-remission"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
