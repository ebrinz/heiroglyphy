{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3054a28f",
   "metadata": {},
   "source": [
    "# lets see if we can get some terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "063aa47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:heiro:Loaded hieroglyphic embeddings: (12773, 300) from cache/embeddings/hieroglyphic_embeddings.pkl\n",
      "INFO:heiro:Loaded tla_english embeddings: (12773, 768) from cache/embeddings/tla_english_embeddings.pkl\n",
      "INFO:heiro:Loaded wikipedia embeddings: (101000, 768) from cache/embeddings/wikipedia_embeddings.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiSpaceVec2VecModel(\n",
       "  (encoders): ModuleList(\n",
       "    (0-1): 2 x Sequential(\n",
       "      (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "      (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (4): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=300, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "      (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (4): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (decoders): ModuleList(\n",
       "    (0-1): 2 x Sequential(\n",
       "      (0): Linear(in_features=256, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "      (3): Linear(in_features=512, out_features=768, bias=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "      (3): Linear(in_features=512, out_features=300, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# --- Setup: Import and Load Models ---\n",
    "from heiro import Config, EmbeddingManager, MultiSpaceVec2VecModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Initialize config and embedding manager\n",
    "config = Config()\n",
    "embedding_manager = EmbeddingManager(config)\n",
    "\n",
    "# Load embeddings\n",
    "hier_data = embedding_manager.load_embeddings('hieroglyphic')\n",
    "tla_english_data = embedding_manager.load_embeddings('tla_english')\n",
    "wikipedia_data = embedding_manager.load_embeddings('wikipedia')\n",
    "\n",
    "# Load trained vec2vec model\n",
    "model_path = config.MODELS_DIR / \"multi_space_vec2vec_model.pt\"\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() and torch.backends.mps.is_built() else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "model = MultiSpaceVec2VecModel(\n",
    "    checkpoint['input_dims'],\n",
    "    checkpoint['latent_dim'],\n",
    "    checkpoint['num_spaces']\n",
    ")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b6c8499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hieroglyphic: ꜥḥꜥ | Expert English: Get up!\n",
      "Latent space similarity (to expert translation): 0.8207\n",
      "\n",
      "Closest Wikipedia text in latent space:\n",
      "  Text: A computer program is a sequence or set of instructions in a programming language for a computer to execute. It is one component of software, which also includes documentation and other intangible components.\n",
      "\n",
      "A computer program in its human-readable form is called source code. Source code needs another computer program to execute because computers can only execute their native machine instructions. Therefore, source code may be translated to machine instructions using the language's compiler. (Assembly language programs are translated using an assembler.) The resulting file is called an executable. Alternatively, source code may execute within the language's interpreter.\n",
      "\n",
      "If the executable is requested for execution, then the operating system loads it into memory and starts a process. The central processing unit will soon switch to this process so it can fetch, decode, and then execute each machine instruction.\n",
      "\n",
      "If the source code is requested for execution, then the operating system loads the corresponding interpreter into memory and starts a process. The interpreter then loads the source code into memory to translate and execute each statement. Running the source code is slower than running an executable. Moreover, the interpreter must be installed on the computer.\n",
      "\n",
      "Example computer program\n",
      "\n",
      "The \"Hello, World!\" program is used to illustrate a language's basic syntax. The syntax of the language BASIC (1964) was intentionally limited to make the language easy to learn. For example, variables are not declared before being used. Also, variables are automatically initialized to zero. Here is an example computer program, in Basic, to average a list of numbers:\n",
      "10 INPUT \"How many numbers to average?\", A\n",
      "20 FOR I = 1 TO A\n",
      "30 INPUT \"Enter number:\", B\n",
      "40 LET C = C + B\n",
      "50 NEXT I\n",
      "60 LET D = C/A\n",
      "70 PRINT \"The average is\", D\n",
      "80 END\n",
      "\n",
      "Once the mechanics of basic computer programming are learned, more sophisticated and powerful languages are available to build large computer syst\n",
      "  Similarity: 0.6947\n",
      "  Distance: 0.3053\n"
     ]
    }
   ],
   "source": [
    "# Choose a term and its expert translation\n",
    "# Example: index = 0 (change as needed)\n",
    "idx = 3\n",
    "hier_word = hier_data['texts'][idx]\n",
    "eng_word = tla_english_data['texts'][idx]\n",
    "print(f\"Hieroglyphic: {hier_word} | Expert English: {eng_word}\")\n",
    "\n",
    "# Get embeddings and move to device\n",
    "hier_emb = torch.FloatTensor(hier_data['embeddings'][idx:idx+1]).to(device)\n",
    "eng_emb = torch.FloatTensor(tla_english_data['embeddings'][idx:idx+1]).to(device)\n",
    "wiki_embs = torch.FloatTensor(wikipedia_data['embeddings']).to(device)\n",
    "wiki_texts = wikipedia_data['texts']\n",
    "\n",
    "# Project into latent space\n",
    "with torch.no_grad():\n",
    "    hier_latent = model.encoders[2](hier_emb)   # 2 = hieroglyphic encoder\n",
    "    eng_latent = model.encoders[1](eng_emb)     # 1 = TLA English encoder\n",
    "    wiki_latents = model.encoders[0](wiki_embs) # 0 = Wikipedia encoder\n",
    "\n",
    "# Compute similarity to expert translation\n",
    "sim = cosine_similarity(hier_latent.cpu().numpy(), eng_latent.cpu().numpy())[0][0]\n",
    "print(f\"Latent space similarity (to expert translation): {sim:.4f}\")\n",
    "\n",
    "# Find closest Wikipedia embedding in latent space\n",
    "wiki_sims = cosine_similarity(hier_latent.cpu().numpy(), wiki_latents.cpu().numpy())[0]\n",
    "best_idx = np.argmax(wiki_sims)\n",
    "best_sim = wiki_sims[best_idx]\n",
    "best_wiki_text = wiki_texts[best_idx]\n",
    "\n",
    "print(f\"\\nClosest Wikipedia text in latent space:\")\n",
    "print(f\"  Text: {best_wiki_text}\")\n",
    "print(f\"  Similarity: {best_sim:.4f}\")\n",
    "print(f\"  Distance: {1 - best_sim:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da_astro-word2vec",
   "language": "python",
   "name": "da_astro-word2vec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
