{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V6 Phase 2: Multimodal Alignment\n",
    "\n",
    "## Goal\n",
    "Combine **HieroBERT's contextual embeddings** with our **768d Visual Embeddings** to create a superior representation for alignment.\n",
    "\n",
    "## Strategy\n",
    "1. **Load Models**: Load the pre-trained HieroBERT and the `visual_embeddings_768d.pkl`.\n",
    "2. **Extract Contextual Vectors**: Pass the anchor words through HieroBERT to get their contextualized representations.\n",
    "3. **Visual Fusion**: Combine the BERT vector with the Visual vector (e.g., `BERT + 0.5 * Visual`).\n",
    "4. **Alignment**: Use Procrustes Analysis to align this multimodal space with English GloVe/BERT embeddings.\n",
    "5. **Evaluation**: Test on the V3/V5 evaluation set.\n",
    "\n",
    "## Inputs\n",
    "- `models/hierobert_small`: Pre-trained HieroBERT.\n",
    "- `data/processed/visual_embeddings_768d.pkl`: Visual features.\n",
    "- `data/processed/anchors.json`: The 2,000+ anchor pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T18:17:51.841484Z",
     "iopub.status.busy": "2025-11-21T18:17:51.841367Z",
     "iopub.status.idle": "2025-11-21T18:17:52.766194Z",
     "shell.execute_reply": "2025-11-21T18:17:52.765805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (4.57.1)\r\n",
      "Requirement already satisfied: torch in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (2.7.1)\r\n",
      "Requirement already satisfied: scikit-learn in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (1.7.0)\r\n",
      "Requirement already satisfied: numpy in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (1.26.4)\r\n",
      "Requirement already satisfied: pandas in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (2.3.0)\r\n",
      "Requirement already satisfied: tqdm in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (4.67.1)\r\n",
      "Requirement already satisfied: filelock in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (3.18.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (0.36.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (2.32.5)\r\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (0.22.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (0.5.3)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.3)\r\n",
      "Requirement already satisfied: setuptools in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from torch) (80.9.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from torch) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from torch) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from scikit-learn) (1.5.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from requests->transformers) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from requests->transformers) (2025.4.26)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch scikit-learn numpy pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T18:17:52.767959Z",
     "iopub.status.busy": "2025-11-21T18:17:52.767849Z",
     "iopub.status.idle": "2025-11-21T18:17:57.410715Z",
     "shell.execute_reply": "2025-11-21T18:17:57.410398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from transformers import BertModel, BertTokenizerFast, BertTokenizer\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Paths\n",
    "MODEL_PATH = Path(\"../models/hierobert_small\")\n",
    "VISUAL_PATH = Path(\"../data/processed/visual_embeddings_768d.pkl\")\n",
    "ANCHORS_PATH = Path(\"../data/processed/anchors.json\")\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T18:17:57.412302Z",
     "iopub.status.busy": "2025-11-21T18:17:57.412062Z",
     "iopub.status.idle": "2025-11-21T18:17:57.665215Z",
     "shell.execute_reply": "2025-11-21T18:17:57.664897Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ../models/hierobert_small and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1071 visual embeddings.\n",
      "HieroBERT loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load Visual Embeddings\n",
    "with open(VISUAL_PATH, 'rb') as f:\n",
    "    visual_embeddings = pickle.load(f)\n",
    "print(f\"Loaded {len(visual_embeddings)} visual embeddings.\")\n",
    "\n",
    "# Load HieroBERT\n",
    "tokenizer = BertTokenizerFast.from_pretrained(str(MODEL_PATH))\n",
    "model = BertModel.from_pretrained(str(MODEL_PATH)).to(device)\n",
    "model.eval()\n",
    "print(\"HieroBERT loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Fusion Function\n",
    "How do we combine a sequence of BERT vectors with static visual vectors?\n",
    "\n",
    "**Approach**:\n",
    "1. Get BERT output for the word (mean pooling of last hidden state).\n",
    "2. Look up visual vectors for each glyph in the word.\n",
    "3. Average the visual vectors.\n",
    "4. Combine: `Final = BERT_Vector + (alpha * Visual_Vector)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T18:17:57.666610Z",
     "iopub.status.busy": "2025-11-21T18:17:57.666496Z",
     "iopub.status.idle": "2025-11-21T18:17:57.669361Z",
     "shell.execute_reply": "2025-11-21T18:17:57.669105Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_multimodal_embedding(text, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Generates a 768d vector combining BERT context and Visual features.\n",
    "    \"\"\"\n",
    "    # 1. BERT Embedding\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Mean pooling of last hidden state (excluding [CLS] and [SEP])\n",
    "    # Shape: [1, seq_len, 768]\n",
    "    bert_vec = outputs.last_hidden_state[0, 1:-1, :].mean(dim=0).cpu().numpy()\n",
    "    \n",
    "    if np.isnan(bert_vec).any():\n",
    "        bert_vec = np.zeros(768)\n",
    "\n",
    "    # 2. Visual Embedding\n",
    "    visual_vecs = []\n",
    "    for char in text:\n",
    "        if char in visual_embeddings:\n",
    "            visual_vecs.append(visual_embeddings[char])\n",
    "            \n",
    "    if visual_vecs:\n",
    "        visual_mean = np.mean(visual_vecs, axis=0)\n",
    "    else:\n",
    "        visual_mean = np.zeros(768)\n",
    "        \n",
    "    # 3. Fusion\n",
    "    final_vec = bert_vec + (alpha * visual_mean)\n",
    "    return final_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Anchor Embeddings & Align\n",
    "We will:\n",
    "1. Load English BERT (`bert-base-uncased`) to get target vectors.\n",
    "2. Generate source (Hiero) and target (English) vectors for all anchors.\n",
    "3. Train a Linear Regression (Procrustes) to map Hiero -> English.\n",
    "4. Evaluate Top-K accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-21T18:17:57.670658Z",
     "iopub.status.busy": "2025-11-21T18:17:57.670522Z",
     "iopub.status.idle": "2025-11-21T18:21:30.434458Z",
     "shell.execute_reply": "2025-11-21T18:21:30.429876Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8541 anchor pairs.\n",
      "Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8541/8541 [04:03<00:00, 35.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings for 8541 pairs.\n",
      "Training alignment...\n",
      "Evaluating...\n",
      "Alignment Results:\n",
      "{\n",
      "  \"Top-1\": 0.005851375073142188,\n",
      "  \"Top-5\": 0.016968987712112346,\n",
      "  \"Top-10\": 0.024575775307197192\n",
      "}\n",
      "Alignment model saved.\n"
     ]
    }
   ],
   "source": [
    "# Load Anchors\n",
    "with open(ANCHORS_PATH, 'r') as f:\n",
    "    anchors = json.load(f)\n",
    "print(f\"Loaded {len(anchors)} anchor pairs.\")\n",
    "\n",
    "# Load English BERT for alignment target\n",
    "en_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "en_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "en_model.eval()\n",
    "\n",
    "def get_english_embedding(text):\n",
    "    inputs = en_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = en_model(**inputs)\n",
    "    # Mean pooling\n",
    "    return outputs.last_hidden_state[0, 1:-1, :].mean(dim=0).cpu().numpy()\n",
    "\n",
    "# Generate Datasets\n",
    "X_hiero = []\n",
    "Y_english = []\n",
    "valid_pairs = []\n",
    "\n",
    "print(\"Generating embeddings...\")\n",
    "for pair in tqdm(anchors):\n",
    "    h_text = pair['hieroglyphic']\n",
    "    e_text = pair['english']\n",
    "    \n",
    "    # Generate Multimodal Hiero Vector\n",
    "    h_vec = get_multimodal_embedding(h_text, alpha=0.5)\n",
    "    \n",
    "    # Generate English BERT Vector\n",
    "    e_vec = get_english_embedding(e_text)\n",
    "    \n",
    "    if not np.isnan(h_vec).any() and not np.isnan(e_vec).any():\n",
    "        X_hiero.append(h_vec)\n",
    "        Y_english.append(e_vec)\n",
    "        valid_pairs.append(pair)\n",
    "\n",
    "X = np.array(X_hiero)\n",
    "Y = np.array(Y_english)\n",
    "\n",
    "print(f\"Generated embeddings for {len(X)} pairs.\")\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Procrustes Alignment (Linear Regression)\n",
    "print(\"Training alignment...\")\n",
    "aligner = LinearRegression(fit_intercept=False)\n",
    "aligner.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(model, X_test, Y_test, k_values=[1, 5, 10]):\n",
    "    Y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Cosine Similarity between all predictions and all targets\n",
    "    sim_matrix = cosine_similarity(Y_pred, Y_test)\n",
    "    \n",
    "    top_k_hits = {k: 0 for k in k_values}\n",
    "    n_test = len(X_test)\n",
    "    \n",
    "    for i in range(n_test):\n",
    "        # Get indices of sorted similarities (descending)\n",
    "        sorted_indices = np.argsort(-sim_matrix[i])\n",
    "        \n",
    "        # Check if the correct index (i) is in the top k\n",
    "        for k in k_values:\n",
    "            if i in sorted_indices[:k]:\n",
    "                top_k_hits[k] += 1\n",
    "                \n",
    "    results = {f\"Top-{k}\": hits/n_test for k, hits in top_k_hits.items()}\n",
    "    return results\n",
    "\n",
    "print(\"Evaluating...\")\n",
    "scores = evaluate(aligner, X_test, Y_test)\n",
    "print(\"Alignment Results:\")\n",
    "print(json.dumps(scores, indent=2))\n",
    "\n",
    "# Save the alignment matrix\n",
    "with open('../models/alignment_matrix.pkl', 'wb') as f:\n",
    "    pickle.dump(aligner, f)\n",
    "print(\"Alignment model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not a very great result here, eh?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
