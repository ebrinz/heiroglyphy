{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# V6 Phase 3: Contrastive Fine-tuning\n",
                "\n",
                "## Goal\n",
                "Fine-tune **HieroBERT** to align with **English BERT** using **Contrastive Learning** (InfoNCE).\n",
                "\n",
                "## Strategy\n",
                "1. **Encoders**:\n",
                "    - **Source**: HieroBERT (Trainable). Fused with Visual Embeddings.\n",
                "    - **Target**: English BERT (Frozen). Provides the \"ground truth\" semantic space.\n",
                "2. **Objective**: Minimize the distance between correct translation pairs $(h_i, e_i)$ and maximize it for negatives.\n",
                "3. **Data**: 8,541 Anchor Pairs.\n",
                "4. **Visual Fusion**: `Hiero_Emb = BERT(text) + MLP(Visual_Mean)`\n",
                "\n",
                "## Inputs\n",
                "- `models/hierobert_small`: Pre-trained HieroBERT.\n",
                "- `data/processed/visual_embeddings_768d.pkl`: Visual features.\n",
                "- `data/processed/anchors.json`: Anchor pairs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: transformers in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (4.57.1)\n",
                        "Requirement already satisfied: torch in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (2.7.1)\n",
                        "Requirement already satisfied: scikit-learn in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (1.7.0)\n",
                        "Requirement already satisfied: numpy in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (1.26.4)\n",
                        "Requirement already satisfied: pandas in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (2.3.0)\n",
                        "Requirement already satisfied: tqdm in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (4.67.1)\n",
                        "Requirement already satisfied: filelock in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
                        "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (0.36.0)\n",
                        "Requirement already satisfied: packaging>=20.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (25.0)\n",
                        "Requirement already satisfied: pyyaml>=5.1 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
                        "Requirement already satisfied: regex!=2019.12.17 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
                        "Requirement already satisfied: requests in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
                        "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
                        "Requirement already satisfied: safetensors>=0.4.3 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
                        "Requirement already satisfied: fsspec>=2023.5.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
                        "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
                        "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.3)\n",
                        "Requirement already satisfied: setuptools in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from torch) (80.9.0)\n",
                        "Requirement already satisfied: sympy>=1.13.3 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from torch) (1.14.0)\n",
                        "Requirement already satisfied: networkx in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from torch) (3.5)\n",
                        "Requirement already satisfied: jinja2 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from torch) (3.1.6)\n",
                        "Requirement already satisfied: scipy>=1.8.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
                        "Requirement already satisfied: joblib>=1.2.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from scikit-learn) (1.5.1)\n",
                        "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
                        "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
                        "Requirement already satisfied: pytz>=2020.1 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from pandas) (2025.2)\n",
                        "Requirement already satisfied: tzdata>=2022.7 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from pandas) (2025.2)\n",
                        "Requirement already satisfied: six>=1.5 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
                        "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
                        "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from requests->transformers) (2025.4.26)\n",
                        "\n",
                        "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
                        "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
                    ]
                }
            ],
            "source": [
                "!pip install transformers torch scikit-learn numpy pandas tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: mps\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import pickle\n",
                "import json\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "from transformers import BertModel, BertTokenizerFast, BertTokenizer\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from tqdm import tqdm\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Paths\n",
                "MODEL_PATH = Path(\"../models/hierobert_small\")\n",
                "VISUAL_PATH = Path(\"../data/processed/visual_embeddings_768d.pkl\")\n",
                "ANCHORS_PATH = Path(\"../data/processed/anchors.json\")\n",
                "\n",
                "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading & Dataset Class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Resources\n",
                "with open(VISUAL_PATH, 'rb') as f:\n",
                "    visual_embeddings = pickle.load(f)\n",
                "\n",
                "with open(ANCHORS_PATH, 'r') as f:\n",
                "    anchors = json.load(f)\n",
                "\n",
                "# Tokenizers\n",
                "hiero_tokenizer = BertTokenizerFast.from_pretrained(str(MODEL_PATH))\n",
                "en_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
                "\n",
                "class MultimodalDataset(Dataset):\n",
                "    def __init__(self, anchors, visual_emb, h_tokenizer, e_tokenizer, max_len=32):\n",
                "        self.anchors = anchors\n",
                "        self.visual_emb = visual_emb\n",
                "        self.h_tokenizer = h_tokenizer\n",
                "        self.e_tokenizer = e_tokenizer\n",
                "        self.max_len = max_len\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.anchors)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        item = self.anchors[idx]\n",
                "        h_text = item['hieroglyphic']\n",
                "        e_text = item['english']\n",
                "\n",
                "        # Hiero Tokenization\n",
                "        h_enc = self.h_tokenizer(h_text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')\n",
                "        \n",
                "        # English Tokenization\n",
                "        e_enc = self.e_tokenizer(e_text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')\n",
                "\n",
                "        # Visual Features (Mean of glyphs)\n",
                "        visual_vecs = []\n",
                "        for char in h_text:\n",
                "            if char in self.visual_emb:\n",
                "                visual_vecs.append(self.visual_emb[char])\n",
                "        \n",
                "        if visual_vecs:\n",
                "            v_vec = np.mean(visual_vecs, axis=0)\n",
                "        else:\n",
                "            v_vec = np.zeros(768)\n",
                "\n",
                "        return {\n",
                "            'h_input_ids': h_enc['input_ids'].squeeze(0),\n",
                "            'h_attention_mask': h_enc['attention_mask'].squeeze(0),\n",
                "            'e_input_ids': e_enc['input_ids'].squeeze(0),\n",
                "            'e_attention_mask': e_enc['attention_mask'].squeeze(0),\n",
                "            'visual_vec': torch.tensor(v_vec, dtype=torch.float32)\n",
                "        }\n",
                "\n",
                "# Split & Loader\n",
                "train_anchors, test_anchors = train_test_split(anchors, test_size=0.1, random_state=42)\n",
                "\n",
                "train_dataset = MultimodalDataset(train_anchors, visual_embeddings, hiero_tokenizer, en_tokenizer)\n",
                "test_dataset = MultimodalDataset(test_anchors, visual_embeddings, hiero_tokenizer, en_tokenizer)\n",
                "\n",
                "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
                "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Model Architecture\n",
                "We define `MultimodalHieroBERT` which takes text and visual inputs and outputs a single embedding."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MultimodalHieroBERT(nn.Module):\n",
                "    def __init__(self, model_path):\n",
                "        super().__init__()\n",
                "        self.bert = BertModel.from_pretrained(str(model_path))\n",
                "        # Adapter to fuse visual features\n",
                "        self.visual_adapter = nn.Sequential(\n",
                "            nn.Linear(768, 768),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(768, 768)\n",
                "        )\n",
                "        # LayerNorm for stability\n",
                "        self.ln = nn.LayerNorm(768)\n",
                "\n",
                "    def forward(self, input_ids, attention_mask, visual_vec):\n",
                "        # Get BERT output\n",
                "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
                "        # Mean pooling (excluding CLS/SEP)\n",
                "        # Mask out padding tokens for mean calculation\n",
                "        # Simple approach: use pooler_output (CLS) or mean of last hidden state\n",
                "        # Let's use CLS for now as it's standard for sentence embeddings in BERT\n",
                "        text_emb = outputs.pooler_output \n",
                "        \n",
                "        # Process visual vec\n",
                "        vis_emb = self.visual_adapter(visual_vec)\n",
                "        \n",
                "        # Fusion: Additive\n",
                "        fused_emb = self.ln(text_emb + vis_emb)\n",
                "        return fused_emb"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Training Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of BertModel were not initialized from the model checkpoint at ../models/hierobert_small and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
                        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
                    ]
                }
            ],
            "source": [
                "# Initialize Models\n",
                "hiero_model = MultimodalHieroBERT(MODEL_PATH).to(device)\n",
                "en_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
                "\n",
                "# Freeze English BERT (Target)\n",
                "for param in en_model.parameters():\n",
                "    param.requires_grad = False\n",
                "en_model.eval()\n",
                "\n",
                "# Optimizer\n",
                "optimizer = optim.AdamW(hiero_model.parameters(), lr=2e-5)\n",
                "\n",
                "# Contrastive Loss (InfoNCE)\n",
                "def contrastive_loss(h_emb, e_emb, temperature=0.1):\n",
                "    # Normalize\n",
                "    h_emb = torch.nn.functional.normalize(h_emb, dim=1)\n",
                "    e_emb = torch.nn.functional.normalize(e_emb, dim=1)\n",
                "    \n",
                "    # Cosine similarity matrix: [batch, batch]\n",
                "    logits = torch.matmul(h_emb, e_emb.T) / temperature\n",
                "    \n",
                "    # Labels: diagonal is the positive pair (0,0), (1,1), etc.\n",
                "    labels = torch.arange(logits.size(0)).to(device)\n",
                "    \n",
                "    loss = nn.CrossEntropyLoss()(logits, labels)\n",
                "    return loss"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting Contrastive Fine-tuning...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1: 100%|██████████| 241/241 [01:06<00:00,  3.63it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1 Loss: 3.3932\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 2: 100%|██████████| 241/241 [00:58<00:00,  4.09it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 2 Loss: 3.3165\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 3: 100%|██████████| 241/241 [00:57<00:00,  4.16it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 3 Loss: 3.2534\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 4: 100%|██████████| 241/241 [00:58<00:00,  4.13it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 4 Loss: 3.1867\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 5: 100%|██████████| 241/241 [00:58<00:00,  4.13it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 5 Loss: 3.1196\n",
                        "Model saved.\n"
                    ]
                }
            ],
            "source": [
                "EPOCHS = 5\n",
                "\n",
                "print(\"Starting Contrastive Fine-tuning...\")\n",
                "for epoch in range(EPOCHS):\n",
                "    hiero_model.train()\n",
                "    total_loss = 0\n",
                "    \n",
                "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
                "        # Move to device\n",
                "        h_ids = batch['h_input_ids'].to(device)\n",
                "        h_mask = batch['h_attention_mask'].to(device)\n",
                "        e_ids = batch['e_input_ids'].to(device)\n",
                "        e_mask = batch['e_attention_mask'].to(device)\n",
                "        v_vec = batch['visual_vec'].to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        # Forward Hiero\n",
                "        h_emb = hiero_model(h_ids, h_mask, v_vec)\n",
                "        \n",
                "        # Forward English (Target)\n",
                "        with torch.no_grad():\n",
                "            e_out = en_model(input_ids=e_ids, attention_mask=e_mask)\n",
                "            e_emb = e_out.pooler_output\n",
                "            \n",
                "        # Loss\n",
                "        loss = contrastive_loss(h_emb, e_emb)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        total_loss += loss.item()\n",
                "        \n",
                "    avg_loss = total_loss / len(train_loader)\n",
                "    print(f\"Epoch {epoch+1} Loss: {avg_loss:.4f}\")\n",
                "\n",
                "# Save Fine-tuned Model\n",
                "torch.save(hiero_model.state_dict(), \"../models/hierobert_contrastive.pth\")\n",
                "print(\"Model saved.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Evaluating on Test Set...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Evaluating: 100%|██████████| 27/27 [00:04<00:00,  6.54it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Contrastive Alignment Results:\n",
                        "{\n",
                        "  \"Top-1\": 0.004678362573099415,\n",
                        "  \"Top-5\": 0.023391812865497075,\n",
                        "  \"Top-10\": 0.047953216374269005\n",
                        "}\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "def evaluate(loader, k_values=[1, 5, 10]):\n",
                "    hiero_model.eval()\n",
                "    all_h_embs = []\n",
                "    all_e_embs = []\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
                "            h_ids = batch['h_input_ids'].to(device)\n",
                "            h_mask = batch['h_attention_mask'].to(device)\n",
                "            e_ids = batch['e_input_ids'].to(device)\n",
                "            e_mask = batch['e_attention_mask'].to(device)\n",
                "            v_vec = batch['visual_vec'].to(device)\n",
                "            \n",
                "            h_emb = hiero_model(h_ids, h_mask, v_vec)\n",
                "            e_out = en_model(input_ids=e_ids, attention_mask=e_mask)\n",
                "            e_emb = e_out.pooler_output\n",
                "            \n",
                "            all_h_embs.append(h_emb.cpu())\n",
                "            all_e_embs.append(e_emb.cpu())\n",
                "            \n",
                "    H = torch.cat(all_h_embs)\n",
                "    E = torch.cat(all_e_embs)\n",
                "    \n",
                "    # Normalize\n",
                "    H = torch.nn.functional.normalize(H, dim=1)\n",
                "    E = torch.nn.functional.normalize(E, dim=1)\n",
                "    \n",
                "    # Similarity Matrix\n",
                "    sim_matrix = torch.matmul(H, E.T).numpy()\n",
                "    \n",
                "    top_k_hits = {k: 0 for k in k_values}\n",
                "    n_test = len(H)\n",
                "    \n",
                "    for i in range(n_test):\n",
                "        sorted_indices = np.argsort(-sim_matrix[i])\n",
                "        for k in k_values:\n",
                "            if i in sorted_indices[:k]:\n",
                "                top_k_hits[k] += 1\n",
                "                \n",
                "    results = {f\"Top-{k}\": hits/n_test for k, hits in top_k_hits.items()}\n",
                "    return results\n",
                "\n",
                "print(\"Evaluating on Test Set...\")\n",
                "scores = evaluate(test_loader)\n",
                "print(\"Contrastive Alignment Results:\")\n",
                "print(json.dumps(scores, indent=2))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python (spontaneous-remission)",
            "language": "python",
            "name": "spontaneous-remission"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
