{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# V6 Phase 2: HieroBERT Pre-training\n",
                "\n",
                "## Goal\n",
                "Train a context-aware **HieroBERT** model on the 104k hieroglyphic texts. \n",
                "This model will learn to predict masked hieroglyphs based on their context, capturing the syntax and semantics of the language.\n",
                "\n",
                "## Architecture: \"HieroBERT-Small\"\n",
                "- **Hidden Size**: 768 (Matches our visual embeddings)\n",
                "- **Layers**: 6 (Reduced from 12 to prevent overfitting on small data)\n",
                "- **Attention Heads**: 12\n",
                "- **Vocab Size**: 30,000 (Learned via WordPiece)\n",
                "\n",
                "## Steps\n",
                "1. Train Tokenizer\n",
                "2. Configure Model\n",
                "3. Prepare Dataset (MLM)\n",
                "4. Train\n",
                "5. Save"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: transformers in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (4.57.1)\n",
                        "Requirement already satisfied: tokenizers in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (0.22.1)\n",
                        "Requirement already satisfied: datasets in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (3.6.0)\n",
                        "Requirement already satisfied: torch in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (2.7.1)\n",
                        "Requirement already satisfied: accelerate in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (1.11.0)\n",
                        "Requirement already satisfied: filelock in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
                        "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (0.36.0)\n",
                        "Requirement already satisfied: numpy>=1.17 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
                        "Requirement already satisfied: packaging>=20.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (25.0)\n",
                        "Requirement already satisfied: pyyaml>=5.1 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
                        "Requirement already satisfied: regex!=2019.12.17 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
                        "Requirement already satisfied: requests in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
                        "Requirement already satisfied: safetensors>=0.4.3 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
                        "Requirement already satisfied: tqdm>=4.27 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
                        "Requirement already satisfied: fsspec>=2023.5.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
                        "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
                        "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.3)\n",
                        "Requirement already satisfied: pyarrow>=15.0.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from datasets) (20.0.0)\n",
                        "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
                        "Requirement already satisfied: pandas in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from datasets) (2.3.0)\n",
                        "Requirement already satisfied: xxhash in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
                        "Requirement already satisfied: multiprocess<0.70.17 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
                        "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.12)\n",
                        "Requirement already satisfied: setuptools in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from torch) (80.9.0)\n",
                        "Requirement already satisfied: sympy>=1.13.3 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from torch) (1.14.0)\n",
                        "Requirement already satisfied: networkx in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from torch) (3.5)\n",
                        "Requirement already satisfied: jinja2 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from torch) (3.1.6)\n",
                        "Requirement already satisfied: psutil in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
                        "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
                        "Requirement already satisfied: aiosignal>=1.1.2 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
                        "Requirement already satisfied: attrs>=17.3.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
                        "Requirement already satisfied: frozenlist>=1.1.1 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
                        "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
                        "Requirement already satisfied: propcache>=0.2.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
                        "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
                        "Requirement already satisfied: idna>=2.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
                        "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from requests->transformers) (2025.4.26)\n",
                        "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
                        "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
                        "Requirement already satisfied: pytz>=2020.1 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
                        "Requirement already satisfied: tzdata>=2022.7 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
                        "Requirement already satisfied: six>=1.5 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
                        "\n",
                        "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
                        "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
                    ]
                }
            ],
            "source": [
                "!pip install transformers tokenizers datasets torch accelerate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: mps\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "from pathlib import Path\n",
                "from tokenizers import BertWordPieceTokenizer\n",
                "from transformers import (\n",
                "    BertConfig,\n",
                "    BertForMaskedLM,\n",
                "    LineByLineTextDataset,\n",
                "    DataCollatorForLanguageModeling,\n",
                "    Trainer,\n",
                "    TrainingArguments\n",
                ")\n",
                "import torch\n",
                "\n",
                "# Paths\n",
                "DATA_PATH = Path(\"../data/raw/hieroglyphic_corpus.txt\")\n",
                "MODEL_DIR = Path(\"../models/hierobert_small\")\n",
                "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "print(f\"Using device: {torch.device('mps' if torch.backends.mps.is_available() else 'cpu')}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Train Tokenizer\n",
                "We need a tokenizer that understands hieroglyphic groupings. We'll use WordPiece."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\n",
                        "\n",
                        "Tokenizer saved.\n"
                    ]
                }
            ],
            "source": [
                "# Initialize tokenizer\n",
                "tokenizer = BertWordPieceTokenizer(\n",
                "    clean_text=True,\n",
                "    handle_chinese_chars=False, # Not Chinese\n",
                "    strip_accents=False, # Keep accents if any (though mostly codes)\n",
                "    lowercase=False # Hieroglyph codes are case sensitive (e.g. A1 vs a1? Actually Gardiner is usually uppercase)\n",
                ")\n",
                "\n",
                "# Train\n",
                "tokenizer.train(\n",
                "    files=[str(DATA_PATH)],\n",
                "    vocab_size=30000,\n",
                "    min_frequency=2,\n",
                "    show_progress=True,\n",
                "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
                ")\n",
                "\n",
                "# Save tokenizer\n",
                "tokenizer.save_model(str(MODEL_DIR))\n",
                "print(\"Tokenizer saved.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Configure Model\n",
                "Defining HieroBERT-Small."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model parameters: 66,584,880\n"
                    ]
                }
            ],
            "source": [
                "config = BertConfig(\n",
                "    vocab_size=30000,\n",
                "    hidden_size=768,\n",
                "    num_hidden_layers=6,\n",
                "    num_attention_heads=12,\n",
                "    intermediate_size=3072,\n",
                "    max_position_embeddings=512,\n",
                "    type_vocab_size=1,\n",
                ")\n",
                "\n",
                "model = BertForMaskedLM(config)\n",
                "print(f\"Model parameters: {model.num_parameters():,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Prepare Dataset\n",
                "Loading the corpus for Masked Language Modeling."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
                        "  warnings.warn(\n"
                    ]
                }
            ],
            "source": [
                "# Load tokenizer as Transformers object\n",
                "tokenizer = BertTokenizerFast.from_pretrained(str(MODEL_DIR), max_len=512)\n",
                "\n",
                "# Create Dataset\n",
                "dataset = LineByLineTextDataset(\n",
                "    tokenizer=tokenizer,\n",
                "    file_path=str(DATA_PATH),\n",
                "    block_size=128 # Short texts mostly\n",
                ")\n",
                "\n",
                "data_collator = DataCollatorForLanguageModeling(\n",
                "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Train"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "training_args = TrainingArguments(\n",
                "    output_dir=str(MODEL_DIR),\n",
                "    overwrite_output_dir=True,\n",
                "    num_train_epochs=3,\n",
                "    per_device_train_batch_size=32,\n",
                "    save_steps=500,\n",
                "    save_total_limit=2,\n",
                "    prediction_loss_only=True,\n",
                "    learning_rate=1e-4,\n",
                "    weight_decay=0.01,\n",
                "    logging_steps=50,\n",
                "    use_mps_device=torch.backends.mps.is_available()\n",
                ")\n",
                "\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    data_collator=data_collator,\n",
                "    train_dataset=dataset,\n",
                ")\n",
                "\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "trainer.save_model(str(MODEL_DIR))\n",
                "print(\"HieroBERT saved successfully!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python (spontaneous-remission)",
            "language": "python",
            "name": "spontaneous-remission"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
