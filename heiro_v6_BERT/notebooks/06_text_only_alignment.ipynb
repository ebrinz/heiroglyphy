{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# V6 Phase 4: Text-Only Alignment\n",
                "\n",
                "## Goal\n",
                "Fine-tune **HieroBERT** (Text Only) to align with **English BERT** using **Contrastive Learning** (InfoNCE).\n",
                "\n",
                "## Strategy\n",
                "1. **Encoders**:\n",
                "    - **Source**: HieroBERT (Trainable). No visual fusion.\n",
                "    - **Target**: English BERT (Frozen).\n",
                "2. **Objective**: Minimize the distance between correct translation pairs $(h_i, e_i)$.\n",
                "3. **Data**: 8,541 Anchor Pairs.\n",
                "4. **Output**: Save fine-tuned model as `heirobert_small_2`.\n",
                "\n",
                "## Inputs\n",
                "- `models/hierobert_small`: Pre-trained HieroBERT.\n",
                "- `data/processed/anchors.json`: Anchor pairs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: transformers in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (4.57.1)\n",
                        "Requirement already satisfied: torch in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (2.7.1)\n",
                        "Requirement already satisfied: scikit-learn in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (1.7.0)\n",
                        "Requirement already satisfied: numpy in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (1.26.4)\n",
                        "Requirement already satisfied: pandas in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (2.3.0)\n",
                        "Requirement already satisfied: tqdm in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (4.67.1)\n",
                        "Requirement already satisfied: filelock in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
                        "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (0.36.0)\n",
                        "Requirement already satisfied: packaging>=20.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (25.0)\n",
                        "Requirement already satisfied: pyyaml>=5.1 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
                        "Requirement already satisfied: regex!=2019.12.17 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
                        "Requirement already satisfied: requests in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
                        "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
                        "Requirement already satisfied: safetensors>=0.4.3 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
                        "Requirement already satisfied: fsspec>=2023.5.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
                        "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
                        "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.3)\n",
                        "Requirement already satisfied: setuptools in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from torch) (80.9.0)\n",
                        "Requirement already satisfied: sympy>=1.13.3 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from torch) (1.14.0)\n",
                        "Requirement already satisfied: networkx in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from torch) (3.5)\n",
                        "Requirement already satisfied: jinja2 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from torch) (3.1.6)\n",
                        "Requirement already satisfied: scipy>=1.8.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
                        "Requirement already satisfied: joblib>=1.2.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from scikit-learn) (1.5.1)\n",
                        "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
                        "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
                        "Requirement already satisfied: pytz>=2020.1 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from pandas) (2025.2)\n",
                        "Requirement already satisfied: tzdata>=2022.7 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from pandas) (2025.2)\n",
                        "Requirement already satisfied: six>=1.5 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
                        "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
                        "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in /Users/crashy/.pyenv/versions/3.12.3/lib/python3.12/site-packages (from requests->transformers) (2025.4.26)\n",
                        "\n",
                        "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
                        "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
                    ]
                }
            ],
            "source": [
                "!pip install transformers torch scikit-learn numpy pandas tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: mps\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import json\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "from transformers import BertModel, BertTokenizerFast, BertTokenizer\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from tqdm import tqdm\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Paths\n",
                "MODEL_PATH = Path(\"../models/hierobert_small\")\n",
                "ANCHORS_PATH = Path(\"../data/processed/anchors.json\")\n",
                "SAVE_PATH = Path(\"../models/heirobert_small_2\")\n",
                "\n",
                "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading & Dataset Class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Anchors\n",
                "with open(ANCHORS_PATH, 'r') as f:\n",
                "    anchors = json.load(f)\n",
                "\n",
                "# Tokenizers\n",
                "hiero_tokenizer = BertTokenizerFast.from_pretrained(str(MODEL_PATH))\n",
                "en_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
                "\n",
                "class TextDataset(Dataset):\n",
                "    def __init__(self, anchors, h_tokenizer, e_tokenizer, max_len=32):\n",
                "        self.anchors = anchors\n",
                "        self.h_tokenizer = h_tokenizer\n",
                "        self.e_tokenizer = e_tokenizer\n",
                "        self.max_len = max_len\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.anchors)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        item = self.anchors[idx]\n",
                "        h_text = item['hieroglyphic']\n",
                "        e_text = item['english']\n",
                "\n",
                "        # Hiero Tokenization\n",
                "        h_enc = self.h_tokenizer(h_text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')\n",
                "        \n",
                "        # English Tokenization\n",
                "        e_enc = self.e_tokenizer(e_text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')\n",
                "\n",
                "        return {\n",
                "            'h_input_ids': h_enc['input_ids'].squeeze(0),\n",
                "            'h_attention_mask': h_enc['attention_mask'].squeeze(0),\n",
                "            'e_input_ids': e_enc['input_ids'].squeeze(0),\n",
                "            'e_attention_mask': e_enc['attention_mask'].squeeze(0)\n",
                "        }\n",
                "\n",
                "# Split & Loader\n",
                "train_anchors, test_anchors = train_test_split(anchors, test_size=0.1, random_state=42)\n",
                "\n",
                "train_dataset = TextDataset(train_anchors, hiero_tokenizer, en_tokenizer)\n",
                "test_dataset = TextDataset(test_anchors, hiero_tokenizer, en_tokenizer)\n",
                "\n",
                "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
                "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Model Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of BertModel were not initialized from the model checkpoint at ../models/hierobert_small and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
                        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
                    ]
                }
            ],
            "source": [
                "# Initialize Models\n",
                "hiero_model = BertModel.from_pretrained(str(MODEL_PATH)).to(device)\n",
                "en_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
                "\n",
                "# Freeze English BERT (Target)\n",
                "for param in en_model.parameters():\n",
                "    param.requires_grad = False\n",
                "en_model.eval()\n",
                "\n",
                "# Optimizer\n",
                "optimizer = optim.AdamW(hiero_model.parameters(), lr=2e-5)\n",
                "\n",
                "# Contrastive Loss (InfoNCE)\n",
                "def contrastive_loss(h_emb, e_emb, temperature=0.1):\n",
                "    # Normalize\n",
                "    h_emb = torch.nn.functional.normalize(h_emb, dim=1)\n",
                "    e_emb = torch.nn.functional.normalize(e_emb, dim=1)\n",
                "    \n",
                "    # Cosine similarity matrix: [batch, batch]\n",
                "    logits = torch.matmul(h_emb, e_emb.T) / temperature\n",
                "    \n",
                "    # Labels: diagonal is the positive pair (0,0), (1,1), etc.\n",
                "    labels = torch.arange(logits.size(0)).to(device)\n",
                "    \n",
                "    loss = nn.CrossEntropyLoss()(logits, labels)\n",
                "    return loss"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting Text-Only Fine-tuning...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1: 100%|██████████| 241/241 [01:07<00:00,  3.57it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1 Loss: 3.3948\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 2: 100%|██████████| 241/241 [00:56<00:00,  4.24it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 2 Loss: 3.3204\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 3: 100%|██████████| 241/241 [00:57<00:00,  4.20it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 3 Loss: 3.2586\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 4: 100%|██████████| 241/241 [00:58<00:00,  4.14it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 4 Loss: 3.2009\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 5: 100%|██████████| 241/241 [00:58<00:00,  4.15it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 5 Loss: 3.1473\n",
                        "Model saved to ../models/heirobert_small_2\n"
                    ]
                }
            ],
            "source": [
                "EPOCHS = 5\n",
                "\n",
                "print(\"Starting Text-Only Fine-tuning...\")\n",
                "for epoch in range(EPOCHS):\n",
                "    hiero_model.train()\n",
                "    total_loss = 0\n",
                "    \n",
                "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
                "        # Move to device\n",
                "        h_ids = batch['h_input_ids'].to(device)\n",
                "        h_mask = batch['h_attention_mask'].to(device)\n",
                "        e_ids = batch['e_input_ids'].to(device)\n",
                "        e_mask = batch['e_attention_mask'].to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        # Forward Hiero\n",
                "        h_out = hiero_model(input_ids=h_ids, attention_mask=h_mask)\n",
                "        h_emb = h_out.pooler_output\n",
                "        \n",
                "        # Forward English (Target)\n",
                "        with torch.no_grad():\n",
                "            e_out = en_model(input_ids=e_ids, attention_mask=e_mask)\n",
                "            e_emb = e_out.pooler_output\n",
                "            \n",
                "        # Loss\n",
                "        loss = contrastive_loss(h_emb, e_emb)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        total_loss += loss.item()\n",
                "        \n",
                "    avg_loss = total_loss / len(train_loader)\n",
                "    print(f\"Epoch {epoch+1} Loss: {avg_loss:.4f}\")\n",
                "\n",
                "# Save Fine-tuned Model\n",
                "SAVE_PATH.mkdir(parents=True, exist_ok=True)\n",
                "hiero_model.save_pretrained(SAVE_PATH)\n",
                "hiero_tokenizer.save_pretrained(SAVE_PATH)\n",
                "print(f\"Model saved to {SAVE_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Evaluating on Test Set...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Evaluating: 100%|██████████| 27/27 [00:03<00:00,  7.50it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Text-Only Alignment Results:\n",
                        "{\n",
                        "  \"Top-1\": 0.0023391812865497076,\n",
                        "  \"Top-5\": 0.026900584795321637,\n",
                        "  \"Top-10\": 0.047953216374269005\n",
                        "}\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "def evaluate(loader, k_values=[1, 5, 10]):\n",
                "    hiero_model.eval()\n",
                "    all_h_embs = []\n",
                "    all_e_embs = []\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
                "            h_ids = batch['h_input_ids'].to(device)\n",
                "            h_mask = batch['h_attention_mask'].to(device)\n",
                "            e_ids = batch['e_input_ids'].to(device)\n",
                "            e_mask = batch['e_attention_mask'].to(device)\n",
                "            \n",
                "            h_out = hiero_model(input_ids=h_ids, attention_mask=h_mask)\n",
                "            e_out = en_model(input_ids=e_ids, attention_mask=e_mask)\n",
                "            \n",
                "            all_h_embs.append(h_out.pooler_output.cpu())\n",
                "            all_e_embs.append(e_out.pooler_output.cpu())\n",
                "            \n",
                "    H = torch.cat(all_h_embs)\n",
                "    E = torch.cat(all_e_embs)\n",
                "    \n",
                "    # Normalize\n",
                "    H = torch.nn.functional.normalize(H, dim=1)\n",
                "    E = torch.nn.functional.normalize(E, dim=1)\n",
                "    \n",
                "    # Similarity Matrix\n",
                "    sim_matrix = torch.matmul(H, E.T).numpy()\n",
                "    \n",
                "    top_k_hits = {k: 0 for k in k_values}\n",
                "    n_test = len(H)\n",
                "    \n",
                "    for i in range(n_test):\n",
                "        sorted_indices = np.argsort(-sim_matrix[i])\n",
                "        for k in k_values:\n",
                "            if i in sorted_indices[:k]:\n",
                "                top_k_hits[k] += 1\n",
                "                \n",
                "    results = {f\"Top-{k}\": hits/n_test for k, hits in top_k_hits.items()}\n",
                "    return results\n",
                "\n",
                "print(\"Evaluating on Test Set...\")\n",
                "scores = evaluate(test_loader)\n",
                "print(\"Text-Only Alignment Results:\")\n",
                "print(json.dumps(scores, indent=2))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python (spontaneous-remission)",
            "language": "python",
            "name": "spontaneous-remission"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
